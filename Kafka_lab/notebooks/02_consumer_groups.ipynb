{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Consumer Groups and Load Balancing\n",
        "\n",
        "## 🎯 Objectives\n",
        "- Understand Consumer Groups and how they work\n",
        "- Learn about partition assignment strategies\n",
        "- Practice load balancing across multiple consumers\n",
        "- Explore consumer group coordination\n",
        "- Monitor consumer lag and performance\n",
        "\n",
        "## 📋 Prerequisites\n",
        "- Lab 1 completed (Kafka basics)\n",
        "- Kafka cluster running\n",
        "- Understanding of partitions and topics\n",
        "\n",
        "## 🏗️ Architecture Overview\n",
        "```\n",
        "Stock Data Topic (3 Partitions)\n",
        "         ↓\n",
        "    Consumer Group A\n",
        "    ├── Consumer 1 (Partition 0)\n",
        "    ├── Consumer 2 (Partition 1)\n",
        "    └── Consumer 3 (Partition 2)\n",
        "    \n",
        "    Consumer Group B\n",
        "    ├── Consumer 1 (All Partitions)\n",
        "    └── Consumer 2 (Standby)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kafka-python in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.2.15)\n",
            "Requirement already satisfied: pandas in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (3.8.2)\n",
            "Requirement already satisfied: seaborn in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: six>=1.5 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "✅ Dependencies installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install kafka-python pandas matplotlib seaborn\n",
        "\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"✅ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Configured for consumer groups:\n",
            "🔗 Kafka Bootstrap Servers: localhost:9092\n",
            "📝 Topic Name: stock-data\n",
            "👥 Analytics Group: stock-analytics-group\n",
            "👥 Alerts Group: stock-alerts-group\n",
            "👥 Storage Group: stock-storage-group\n"
          ]
        }
      ],
      "source": [
        "# Kafka Configuration\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "TOPIC_NAME = 'stock-data'\n",
        "\n",
        "# Consumer Group Names\n",
        "ANALYTICS_GROUP = 'stock-analytics-group'\n",
        "ALERTS_GROUP = 'stock-alerts-group'\n",
        "STORAGE_GROUP = 'stock-storage-group'\n",
        "\n",
        "print(f\"📊 Configured for consumer groups:\")\n",
        "print(f\"🔗 Kafka Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "print(f\"📝 Topic Name: {TOPIC_NAME}\")\n",
        "print(f\"👥 Analytics Group: {ANALYTICS_GROUP}\")\n",
        "print(f\"👥 Alerts Group: {ALERTS_GROUP}\")\n",
        "print(f\"👥 Storage Group: {STORAGE_GROUP}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Stock data generator initialized!\n"
          ]
        }
      ],
      "source": [
        "# Stock Data Generator Class\n",
        "class StockDataGenerator:\n",
        "    def __init__(self, bootstrap_servers: str, topic: str):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers=bootstrap_servers,\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "            key_serializer=lambda k: k.encode('utf-8') if k else None\n",
        "        )\n",
        "        self.topic = topic\n",
        "        self.symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'META', 'NVDA', 'NFLX', 'ADBE', 'CRM']\n",
        "        self.base_prices = {\n",
        "            'AAPL': 150.0, 'GOOGL': 2800.0, 'MSFT': 350.0, 'TSLA': 250.0, 'AMZN': 3200.0,\n",
        "            'META': 300.0, 'NVDA': 450.0, 'NFLX': 400.0, 'ADBE': 500.0, 'CRM': 200.0\n",
        "        }\n",
        "    \n",
        "    def generate_ohlcv_data(self, symbol: str, base_price: float) -> dict:\n",
        "        \"\"\"Generate realistic OHLCV data\"\"\"\n",
        "        price_change = random.uniform(-0.02, 0.02)\n",
        "        new_price = base_price * (1 + price_change)\n",
        "        \n",
        "        open_price = round(new_price * random.uniform(0.998, 1.002), 2)\n",
        "        close_price = round(new_price * random.uniform(0.998, 1.002), 2)\n",
        "        high_price = round(max(open_price, close_price) * random.uniform(1.001, 1.005), 2)\n",
        "        low_price = round(min(open_price, close_price) * random.uniform(0.995, 0.999), 2)\n",
        "        \n",
        "        volume = random.randint(100000, 1000000)\n",
        "        \n",
        "        return {\n",
        "            \"symbol\": symbol,\n",
        "            \"timestamp\": datetime.now().isoformat() + \"Z\",\n",
        "            \"open\": open_price,\n",
        "            \"high\": high_price,\n",
        "            \"low\": low_price,\n",
        "            \"close\": close_price,\n",
        "            \"volume\": volume,\n",
        "            \"exchange\": \"NASDAQ\"\n",
        "        }\n",
        "    \n",
        "    def send_stock_data(self, num_messages: int = 10):\n",
        "        \"\"\"Send stock data to Kafka topic\"\"\"\n",
        "        print(f\"📈 Generating {num_messages} stock data messages...\")\n",
        "        \n",
        "        for i in range(num_messages):\n",
        "            symbol = random.choice(self.symbols)\n",
        "            base_price = self.base_prices[symbol]\n",
        "            ohlcv_data = self.generate_ohlcv_data(symbol, base_price)\n",
        "            \n",
        "            # Use symbol as key for partitioning\n",
        "            future = self.producer.send(self.topic, key=symbol, value=ohlcv_data)\n",
        "            record_metadata = future.get(timeout=10)\n",
        "            \n",
        "            print(f\"📊 Sent {symbol}: ${ohlcv_data['close']} -> Partition {record_metadata.partition}\")\n",
        "            time.sleep(0.1)  # Small delay between messages\n",
        "        \n",
        "        self.producer.flush()\n",
        "        print(f\"✅ Successfully sent {num_messages} messages to topic '{self.topic}'\")\n",
        "\n",
        "# Initialize data generator\n",
        "data_generator = StockDataGenerator(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME)\n",
        "print(\"✅ Stock data generator initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Understanding Consumer Groups\n",
        "\n",
        "### 🎯 **Learning Objectives:**\n",
        "- Understand how Consumer Groups work\n",
        "- Learn about partition assignment strategies\n",
        "- Observe load balancing in action\n",
        "- Monitor consumer group behavior\n",
        "\n",
        "### 📚 **Key Concepts:**\n",
        "1. **Consumer Group**: A collection of consumers that work together to consume messages from topics\n",
        "2. **Partition Assignment**: Each partition is assigned to only one consumer in a group\n",
        "3. **Load Balancing**: Messages are distributed across consumers in the group\n",
        "4. **Group Coordination**: Kafka coordinates partition assignments automatically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Creating multiple consumers in the same group...\n",
            "✅ Created 2 consumers in analytics group\n",
            "👥 Group: stock-analytics-group\n",
            "🔧 Consumer 1: analytics-consumer-1\n",
            "🔧 Consumer 2: analytics-consumer-2\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Create Multiple Consumers in Same Group\n",
        "print(\"🔧 Creating multiple consumers in the same group...\")\n",
        "\n",
        "def create_consumer(group_id: str, consumer_id: str):\n",
        "    \"\"\"Create a consumer with specific group and ID\"\"\"\n",
        "    return KafkaConsumer(\n",
        "        TOPIC_NAME,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        group_id=group_id,\n",
        "        client_id=consumer_id,\n",
        "        auto_offset_reset='earliest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        consumer_timeout_ms=5000  # Timeout after 5 seconds if no messages\n",
        "    )\n",
        "\n",
        "# Create consumers for analytics group\n",
        "analytics_consumer_1 = create_consumer(ANALYTICS_GROUP, 'analytics-consumer-1')\n",
        "analytics_consumer_2 = create_consumer(ANALYTICS_GROUP, 'analytics-consumer-2')\n",
        "\n",
        "print(\"✅ Created 2 consumers in analytics group\")\n",
        "print(f\"👥 Group: {ANALYTICS_GROUP}\")\n",
        "print(f\"🔧 Consumer 1: analytics-consumer-1\")\n",
        "print(f\"🔧 Consumer 2: analytics-consumer-2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📈 Generating test stock data...\n",
            "📈 Generating 20 stock data messages...\n",
            "📊 Sent TSLA: $253.59 -> Partition 0\n",
            "📊 Sent AAPL: $150.35 -> Partition 0\n",
            "📊 Sent ADBE: $500.5 -> Partition 0\n",
            "📊 Sent NFLX: $406.37 -> Partition 0\n",
            "📊 Sent CRM: $203.24 -> Partition 0\n",
            "📊 Sent GOOGL: $2812.76 -> Partition 0\n",
            "📊 Sent META: $302.7 -> Partition 0\n",
            "📊 Sent NVDA: $455.2 -> Partition 0\n",
            "📊 Sent MSFT: $348.61 -> Partition 0\n",
            "📊 Sent NVDA: $451.56 -> Partition 0\n",
            "📊 Sent ADBE: $496.94 -> Partition 0\n",
            "📊 Sent ADBE: $494.78 -> Partition 0\n",
            "📊 Sent META: $305.15 -> Partition 0\n",
            "📊 Sent GOOGL: $2849.01 -> Partition 0\n",
            "📊 Sent CRM: $196.23 -> Partition 0\n",
            "📊 Sent MSFT: $351.62 -> Partition 0\n",
            "📊 Sent NVDA: $442.08 -> Partition 0\n",
            "📊 Sent CRM: $201.02 -> Partition 0\n",
            "📊 Sent GOOGL: $2817.01 -> Partition 0\n",
            "📊 Sent AAPL: $152.06 -> Partition 0\n",
            "✅ Successfully sent 20 messages to topic 'stock-data'\n",
            "✅ Test data generated!\n",
            "💡 Now let's see how consumers handle the messages...\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: Generate Test Data\n",
        "print(\"📈 Generating test stock data...\")\n",
        "\n",
        "# Send some test data\n",
        "data_generator.send_stock_data(20)\n",
        "\n",
        "print(\"✅ Test data generated!\")\n",
        "print(\"💡 Now let's see how consumers handle the messages...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Demonstrating consumer group load balancing...\n",
            "\n",
            "🔄 Starting Consumer 1...\n",
            "🚀 Analytics Consumer 1 starting to consume messages...\n",
            "📊 Analytics Consumer 1 received AAPL: $147.29 from partition 0\n",
            "📊 Analytics Consumer 1 received CRM: $200.24 from partition 0\n",
            "📊 Analytics Consumer 1 received MSFT: $353.31 from partition 0\n",
            "📊 Analytics Consumer 1 received AMZN: $3193.57 from partition 0\n",
            "📊 Analytics Consumer 1 received META: $300.17 from partition 0\n",
            "📊 Analytics Consumer 1 received TSLA: $249.58 from partition 0\n",
            "📊 Analytics Consumer 1 received NVDA: $453.01 from partition 0\n",
            "📊 Analytics Consumer 1 received NFLX: $401.11 from partition 0\n",
            "📊 Analytics Consumer 1 received TSLA: $248.7 from partition 0\n",
            "📊 Analytics Consumer 1 received NFLX: $401.48 from partition 0\n",
            "📈 Analytics Consumer 1 summary:\n",
            "   Total messages: 10\n",
            "   Partition distribution: {0: 10}\n",
            "\n",
            "🔄 Starting Consumer 2...\n",
            "🚀 Analytics Consumer 2 starting to consume messages...\n",
            "📈 Analytics Consumer 2 summary:\n",
            "   Total messages: 0\n",
            "   Partition distribution: {}\n",
            "\n",
            "📊 Load Balancing Results:\n",
            "Consumer 1 partitions: {0: 10}\n",
            "Consumer 2 partitions: {}\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: Consumer Group Load Balancing Demo\n",
        "print(\"🔄 Demonstrating consumer group load balancing...\")\n",
        "\n",
        "def consume_messages(consumer, consumer_name: str, max_messages: int = 10):\n",
        "    \"\"\"Consume messages and track partition assignments\"\"\"\n",
        "    messages_received = 0\n",
        "    partition_counts = {}\n",
        "    \n",
        "    print(f\"🚀 {consumer_name} starting to consume messages...\")\n",
        "    \n",
        "    try:\n",
        "        for message in consumer:\n",
        "            if messages_received >= max_messages:\n",
        "                break\n",
        "                \n",
        "            partition = message.partition\n",
        "            partition_counts[partition] = partition_counts.get(partition, 0) + 1\n",
        "            \n",
        "            data = message.value\n",
        "            print(f\"📊 {consumer_name} received {data['symbol']}: ${data['close']} from partition {partition}\")\n",
        "            \n",
        "            messages_received += 1\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ {consumer_name} finished consuming: {e}\")\n",
        "    \n",
        "    print(f\"📈 {consumer_name} summary:\")\n",
        "    print(f\"   Total messages: {messages_received}\")\n",
        "    print(f\"   Partition distribution: {partition_counts}\")\n",
        "    \n",
        "    return partition_counts\n",
        "\n",
        "# Start consuming with both consumers\n",
        "print(\"\\n🔄 Starting Consumer 1...\")\n",
        "partition_counts_1 = consume_messages(analytics_consumer_1, \"Analytics Consumer 1\", 10)\n",
        "\n",
        "print(\"\\n🔄 Starting Consumer 2...\")\n",
        "partition_counts_2 = consume_messages(analytics_consumer_2, \"Analytics Consumer 2\", 10)\n",
        "\n",
        "print(\"\\n📊 Load Balancing Results:\")\n",
        "print(f\"Consumer 1 partitions: {partition_counts_1}\")\n",
        "print(f\"Consumer 2 partitions: {partition_counts_2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Multiple Consumer Groups\n",
        "\n",
        "### 🎯 **Learning Objectives:**\n",
        "- Understand how different consumer groups work independently\n",
        "- Learn about message replication across groups\n",
        "- Observe group coordination and rebalancing\n",
        "\n",
        "### 📚 **Key Concepts:**\n",
        "1. **Independent Groups**: Each consumer group processes all messages independently\n",
        "2. **Message Replication**: Same message can be consumed by multiple groups\n",
        "3. **Group Isolation**: Groups don't interfere with each other's processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Creating consumers for different groups...\n",
            "✅ Created consumers for different groups:\n",
            "🚨 Alerts Group: stock-alerts-group\n",
            "💾 Storage Group: stock-storage-group\n",
            "\n",
            "📈 Generating more test data...\n",
            "📈 Generating 15 stock data messages...\n",
            "📊 Sent ADBE: $491.52 -> Partition 0\n",
            "📊 Sent NVDA: $453.9 -> Partition 0\n",
            "📊 Sent NFLX: $395.22 -> Partition 0\n",
            "📊 Sent GOOGL: $2786.11 -> Partition 0\n",
            "📊 Sent NVDA: $447.54 -> Partition 0\n",
            "📊 Sent CRM: $202.09 -> Partition 0\n",
            "📊 Sent MSFT: $346.8 -> Partition 0\n",
            "📊 Sent AMZN: $3244.14 -> Partition 0\n",
            "📊 Sent NVDA: $445.78 -> Partition 0\n",
            "📊 Sent AMZN: $3220.14 -> Partition 0\n",
            "📊 Sent MSFT: $351.88 -> Partition 0\n",
            "📊 Sent NFLX: $404.98 -> Partition 0\n",
            "📊 Sent MSFT: $354.27 -> Partition 0\n",
            "📊 Sent TSLA: $253.69 -> Partition 0\n",
            "📊 Sent NFLX: $403.07 -> Partition 0\n",
            "✅ Successfully sent 15 messages to topic 'stock-data'\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: Create Multiple Consumer Groups\n",
        "print(\"🔧 Creating consumers for different groups...\")\n",
        "\n",
        "# Create consumers for different groups\n",
        "alerts_consumer = create_consumer(ALERTS_GROUP, 'alerts-consumer')\n",
        "storage_consumer = create_consumer(STORAGE_GROUP, 'storage-consumer')\n",
        "\n",
        "print(\"✅ Created consumers for different groups:\")\n",
        "print(f\"🚨 Alerts Group: {ALERTS_GROUP}\")\n",
        "print(f\"💾 Storage Group: {STORAGE_GROUP}\")\n",
        "\n",
        "# Generate more test data\n",
        "print(\"\\n📈 Generating more test data...\")\n",
        "data_generator.send_stock_data(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Demonstrating independent group processing...\n",
            "\n",
            "🚨 Alerts Group Processing:\n",
            "   📊 AAPL: $147.29 - No alert\n",
            "   🚨 HIGH PRICE ALERT: CRM at $200.24\n",
            "   🚨 HIGH PRICE ALERT: MSFT at $353.31\n",
            "   🚨 HIGH PRICE ALERT: AMZN at $3193.57\n",
            "   🚨 HIGH PRICE ALERT: META at $300.17\n",
            "\n",
            "💾 Storage Group Processing:\n",
            "   💾 STORED: AAPL at 2025-09-21T15:56:47.006979Z\n",
            "   💾 STORED: CRM at 2025-09-21T16:04:43.742567Z\n",
            "   💾 STORED: MSFT at 2025-09-21T16:04:43.977061Z\n",
            "   💾 STORED: AMZN at 2025-09-21T16:04:44.094246Z\n",
            "   💾 STORED: META at 2025-09-21T16:04:44.215153Z\n",
            "\n",
            "✅ Both groups processed messages independently!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 5: Demonstrate Independent Group Processing\n",
        "print(\"🔄 Demonstrating independent group processing...\")\n",
        "\n",
        "def process_alerts(message_data):\n",
        "    \"\"\"Process stock data for alerts\"\"\"\n",
        "    symbol = message_data['symbol']\n",
        "    close_price = message_data['close']\n",
        "    \n",
        "    # Simple alert logic: alert if price change is significant\n",
        "    if close_price > 200:  # High price alert\n",
        "        return f\"🚨 HIGH PRICE ALERT: {symbol} at ${close_price}\"\n",
        "    return None\n",
        "\n",
        "def process_storage(message_data):\n",
        "    \"\"\"Process stock data for storage\"\"\"\n",
        "    symbol = message_data['symbol']\n",
        "    timestamp = message_data['timestamp']\n",
        "    \n",
        "    # Simulate storing to database\n",
        "    return f\"💾 STORED: {symbol} at {timestamp}\"\n",
        "\n",
        "# Process messages with different groups\n",
        "print(\"\\n🚨 Alerts Group Processing:\")\n",
        "alerts_processed = 0\n",
        "for message in alerts_consumer:\n",
        "    if alerts_processed >= 5:\n",
        "        break\n",
        "    \n",
        "    data = message.value\n",
        "    alert = process_alerts(data)\n",
        "    if alert:\n",
        "        print(f\"   {alert}\")\n",
        "    else:\n",
        "        print(f\"   📊 {data['symbol']}: ${data['close']} - No alert\")\n",
        "    \n",
        "    alerts_processed += 1\n",
        "\n",
        "print(\"\\n💾 Storage Group Processing:\")\n",
        "storage_processed = 0\n",
        "for message in storage_consumer:\n",
        "    if storage_processed >= 5:\n",
        "        break\n",
        "    \n",
        "    data = message.value\n",
        "    storage_result = process_storage(data)\n",
        "    print(f\"   {storage_result}\")\n",
        "    \n",
        "    storage_processed += 1\n",
        "\n",
        "print(\"\\n✅ Both groups processed messages independently!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Consumer Group Monitoring\n",
        "\n",
        "### 🎯 **Learning Objectives:**\n",
        "- Learn how to monitor consumer groups\n",
        "- Understand consumer lag and performance metrics\n",
        "- Practice troubleshooting consumer issues\n",
        "\n",
        "### 📚 **Key Concepts:**\n",
        "1. **Consumer Lag**: Difference between producer and consumer offsets\n",
        "2. **Group Coordination**: How Kafka manages group membership\n",
        "3. **Rebalancing**: Automatic redistribution of partitions when consumers join/leave\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Monitoring consumer groups...\n",
            "🔍 Monitoring consumer groups...\n",
            "📋 Available Consumer Groups:\n",
            "   👥 Group: stock-alerts-group, Type: consumer\n",
            "   👥 Group: stock-analytics-group, Type: consumer\n",
            "   👥 Group: stock-storage-group, Type: consumer\n",
            "\n",
            "📊 Monitoring group: stock-analytics-group\n",
            "\n",
            "📊 Partition info for topic 'stock-data':\n",
            "   Partitions: {0}\n",
            "\n",
            "📈 Committed offsets for group 'stock-analytics-group':\n",
            "   No committed offsets found for group 'stock-analytics-group'\n",
            "\n",
            "📊 Monitoring group: stock-alerts-group\n",
            "\n",
            "📊 Partition info for topic 'stock-data':\n",
            "   Partitions: {0}\n",
            "\n",
            "📈 Committed offsets for group 'stock-alerts-group':\n",
            "   No committed offsets found for group 'stock-alerts-group'\n",
            "\n",
            "📊 Monitoring group: stock-storage-group\n",
            "\n",
            "📊 Partition info for topic 'stock-data':\n",
            "   Partitions: {0}\n",
            "\n",
            "📈 Committed offsets for group 'stock-storage-group':\n",
            "   No committed offsets found for group 'stock-storage-group'\n",
            "\n",
            "✅ Consumer group monitoring completed!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 6: Monitor Consumer Groups\n",
        "print(\"📊 Monitoring consumer groups...\")\n",
        "\n",
        "from kafka.admin import KafkaAdminClient, ConfigResource, ConfigResourceType\n",
        "\n",
        "def get_consumer_group_info():\n",
        "    \"\"\"Get information about consumer groups\"\"\"\n",
        "    try:\n",
        "        admin_client = KafkaAdminClient(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            client_id='monitor-client'\n",
        "        )\n",
        "        \n",
        "        # List all consumer groups\n",
        "        groups = admin_client.list_consumer_groups()\n",
        "        print(\"📋 Available Consumer Groups:\")\n",
        "        for group in groups:\n",
        "            print(f\"   👥 Group: {group[0]}, Type: {group[1]}\")\n",
        "        \n",
        "        return groups\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error getting consumer group info: {e}\")\n",
        "        return []\n",
        "\n",
        "def monitor_group_offsets(group_id: str):\n",
        "    \"\"\"Monitor offsets for a specific consumer group\"\"\"\n",
        "    try:\n",
        "        consumer = KafkaConsumer(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            group_id=group_id,\n",
        "            enable_auto_commit=False\n",
        "        )\n",
        "        \n",
        "        # Get partition info for the topic\n",
        "        partitions = consumer.partitions_for_topic(TOPIC_NAME)\n",
        "        print(f\"\\n📊 Partition info for topic '{TOPIC_NAME}':\")\n",
        "        print(f\"   Partitions: {partitions}\")\n",
        "        \n",
        "        # Get committed offsets\n",
        "        if partitions:\n",
        "            from kafka import TopicPartition\n",
        "            topic_partitions = [TopicPartition(TOPIC_NAME, p) for p in partitions]\n",
        "            committed_offsets = consumer.committed(*topic_partitions)\n",
        "            \n",
        "            print(f\"\\n📈 Committed offsets for group '{group_id}':\")\n",
        "            if committed_offsets is not None:\n",
        "                for tp, offset in zip(topic_partitions, committed_offsets):\n",
        "                    if offset is not None:\n",
        "                        print(f\"   Partition {tp.partition}: {offset}\")\n",
        "                    else:\n",
        "                        print(f\"   Partition {tp.partition}: No committed offset\")\n",
        "            else:\n",
        "                print(f\"   No committed offsets found for group '{group_id}'\")\n",
        "        \n",
        "        consumer.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error monitoring offsets: {e}\")\n",
        "\n",
        "# Monitor all our consumer groups\n",
        "print(\"🔍 Monitoring consumer groups...\")\n",
        "groups = get_consumer_group_info()\n",
        "\n",
        "for group_id in [ANALYTICS_GROUP, ALERTS_GROUP, STORAGE_GROUP]:\n",
        "    print(f\"\\n📊 Monitoring group: {group_id}\")\n",
        "    monitor_group_offsets(group_id)\n",
        "\n",
        "print(\"\\n✅ Consumer group monitoring completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Advanced Consumer Group Scenarios\n",
        "\n",
        "### 🎯 **Learning Objectives:**\n",
        "- Practice consumer group rebalancing\n",
        "- Understand partition assignment strategies\n",
        "- Learn about consumer group coordination\n",
        "\n",
        "### 📚 **Key Concepts:**\n",
        "1. **Rebalancing**: Automatic redistribution when consumers join/leave\n",
        "2. **Assignment Strategies**: Range, Round Robin, Sticky\n",
        "3. **Group Coordination**: How Kafka manages group membership\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Demonstrating consumer group rebalancing...\n",
            "🔧 Creating consumers for rebalancing demo group: rebalance-demo-group\n",
            "\n",
            "📊 Step 1: Starting with 1 consumer...\n",
            "📈 Generating 10 stock data messages...\n",
            "📊 Sent AAPL: $147.97 -> Partition 0\n",
            "📊 Sent NFLX: $399.17 -> Partition 0\n",
            "📊 Sent AMZN: $3203.71 -> Partition 0\n",
            "📊 Sent TSLA: $250.08 -> Partition 0\n",
            "📊 Sent AAPL: $151.21 -> Partition 0\n",
            "📊 Sent ADBE: $493.91 -> Partition 0\n",
            "📊 Sent NFLX: $404.8 -> Partition 0\n",
            "📊 Sent CRM: $202.58 -> Partition 0\n",
            "📊 Sent MSFT: $355.67 -> Partition 0\n",
            "📊 Sent MSFT: $356.81 -> Partition 0\n",
            "✅ Successfully sent 10 messages to topic 'stock-data'\n",
            "\n",
            "🔄 Consumer 1 consuming messages...\n",
            "\n",
            "📈 Consumer 1 processed 0 messages\n",
            "\n",
            "📊 Step 2: Adding second consumer (rebalancing)...\n",
            "📈 Generating 10 stock data messages...\n",
            "📊 Sent ADBE: $492.93 -> Partition 0\n",
            "📊 Sent MSFT: $349.15 -> Partition 0\n",
            "📊 Sent NVDA: $454.69 -> Partition 0\n",
            "📊 Sent MSFT: $353.11 -> Partition 0\n",
            "📊 Sent AMZN: $3158.54 -> Partition 0\n",
            "📊 Sent ADBE: $489.56 -> Partition 0\n",
            "📊 Sent ADBE: $510.22 -> Partition 0\n",
            "📊 Sent META: $302.72 -> Partition 0\n",
            "📊 Sent GOOGL: $2798.07 -> Partition 0\n",
            "📊 Sent ADBE: $491.87 -> Partition 0\n",
            "✅ Successfully sent 10 messages to topic 'stock-data'\n",
            "\n",
            "🔄 Both consumers processing messages...\n",
            "\n",
            "📈 Consumer 2 processed 0 messages\n",
            "\n",
            "✅ Rebalancing demonstration completed!\n",
            "🧹 Consumers closed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 7: Consumer Group Rebalancing Demo\n",
        "print(\"🔄 Demonstrating consumer group rebalancing...\")\n",
        "\n",
        "def create_consumer_with_assignment_strategy(group_id: str, consumer_id: str, strategy: str = 'range'):\n",
        "    \"\"\"Create consumer with specific assignment strategy\"\"\"\n",
        "    from kafka.coordinator.assignors.range import RangePartitionAssignor\n",
        "    from kafka.coordinator.assignors.roundrobin import RoundRobinPartitionAssignor\n",
        "    from kafka.coordinator.assignors.sticky.sticky_assignor import StickyPartitionAssignor\n",
        "    \n",
        "    # Map strategy names to actual assignor classes\n",
        "    assignor_map = {\n",
        "        'range': RangePartitionAssignor,\n",
        "        'roundrobin': RoundRobinPartitionAssignor,\n",
        "        'sticky': StickyPartitionAssignor\n",
        "    }\n",
        "    \n",
        "    assignor_class = assignor_map.get(strategy, RangePartitionAssignor)\n",
        "    \n",
        "    return KafkaConsumer(\n",
        "        TOPIC_NAME,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        group_id=group_id,\n",
        "        client_id=consumer_id,\n",
        "        auto_offset_reset='earliest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        consumer_timeout_ms=3000,\n",
        "        # Assignment strategy - use class objects, not strings\n",
        "        partition_assignment_strategy=[assignor_class()]\n",
        "    )\n",
        "\n",
        "# Create a new group for rebalancing demo\n",
        "REBALANCE_GROUP = 'rebalance-demo-group'\n",
        "\n",
        "print(f\"🔧 Creating consumers for rebalancing demo group: {REBALANCE_GROUP}\")\n",
        "\n",
        "# Start with 1 consumer\n",
        "print(\"\\n📊 Step 1: Starting with 1 consumer...\")\n",
        "consumer_1 = create_consumer_with_assignment_strategy(REBALANCE_GROUP, 'rebalance-consumer-1')\n",
        "\n",
        "# Generate some data\n",
        "data_generator.send_stock_data(10)\n",
        "\n",
        "# Consume with 1 consumer\n",
        "print(\"\\n🔄 Consumer 1 consuming messages...\")\n",
        "messages_1 = []\n",
        "for message in consumer_1:\n",
        "    if len(messages_1) >= 5:\n",
        "        break\n",
        "    messages_1.append(message)\n",
        "    print(f\"   Consumer 1: {message.value['symbol']} from partition {message.partition}\")\n",
        "\n",
        "print(f\"\\n📈 Consumer 1 processed {len(messages_1)} messages\")\n",
        "\n",
        "# Add second consumer (this will trigger rebalancing)\n",
        "print(\"\\n📊 Step 2: Adding second consumer (rebalancing)...\")\n",
        "consumer_2 = create_consumer_with_assignment_strategy(REBALANCE_GROUP, 'rebalance-consumer-2')\n",
        "\n",
        "# Generate more data\n",
        "data_generator.send_stock_data(10)\n",
        "\n",
        "# Both consumers should now share the load\n",
        "print(\"\\n🔄 Both consumers processing messages...\")\n",
        "messages_2 = []\n",
        "for message in consumer_2:\n",
        "    if len(messages_2) >= 5:\n",
        "        break\n",
        "    messages_2.append(message)\n",
        "    print(f\"   Consumer 2: {message.value['symbol']} from partition {message.partition}\")\n",
        "\n",
        "print(f\"\\n📈 Consumer 2 processed {len(messages_2)} messages\")\n",
        "print(\"\\n✅ Rebalancing demonstration completed!\")\n",
        "\n",
        "# Clean up consumers\n",
        "consumer_1.close()\n",
        "consumer_2.close()\n",
        "print(\"🧹 Consumers closed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 8: Performance Analysis and Visualization\n",
        "\n",
        "### 🎯 **Learning Objectives:**\n",
        "- Analyze consumer group performance\n",
        "- Visualize message distribution across partitions\n",
        "- Understand throughput and latency patterns\n",
        "\n",
        "### 📚 **Key Concepts:**\n",
        "1. **Throughput**: Messages processed per second\n",
        "2. **Latency**: Time between message production and consumption\n",
        "3. **Partition Distribution**: How messages are distributed across partitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Analyzing consumer group performance...\n",
            "📈 Generating performance test data...\n",
            "📈 Generating 30 stock data messages...\n",
            "📊 Sent NVDA: $446.58 -> Partition 0\n",
            "📊 Sent AMZN: $3226.59 -> Partition 0\n",
            "📊 Sent MSFT: $350.28 -> Partition 0\n",
            "📊 Sent AMZN: $3172.82 -> Partition 0\n",
            "📊 Sent MSFT: $354.98 -> Partition 0\n",
            "📊 Sent NFLX: $402.95 -> Partition 0\n",
            "📊 Sent TSLA: $252.99 -> Partition 0\n",
            "📊 Sent GOOGL: $2765.03 -> Partition 0\n",
            "📊 Sent GOOGL: $2765.22 -> Partition 0\n",
            "📊 Sent META: $299.13 -> Partition 0\n",
            "📊 Sent TSLA: $248.2 -> Partition 0\n",
            "📊 Sent ADBE: $498.65 -> Partition 0\n",
            "📊 Sent MSFT: $351.08 -> Partition 0\n",
            "📊 Sent MSFT: $353.98 -> Partition 0\n",
            "📊 Sent TSLA: $245.12 -> Partition 0\n",
            "📊 Sent ADBE: $504.24 -> Partition 0\n",
            "📊 Sent MSFT: $355.81 -> Partition 0\n",
            "📊 Sent TSLA: $249.55 -> Partition 0\n",
            "📊 Sent AMZN: $3220.47 -> Partition 0\n",
            "📊 Sent AAPL: $148.31 -> Partition 0\n",
            "📊 Sent TSLA: $245.2 -> Partition 0\n",
            "📊 Sent META: $304.41 -> Partition 0\n",
            "📊 Sent TSLA: $247.19 -> Partition 0\n",
            "📊 Sent AMZN: $3244.28 -> Partition 0\n",
            "📊 Sent AAPL: $150.75 -> Partition 0\n",
            "📊 Sent TSLA: $245.35 -> Partition 0\n",
            "📊 Sent MSFT: $351.56 -> Partition 0\n",
            "📊 Sent NFLX: $404.44 -> Partition 0\n",
            "📊 Sent CRM: $195.95 -> Partition 0\n",
            "📊 Sent NFLX: $404.11 -> Partition 0\n",
            "✅ Successfully sent 30 messages to topic 'stock-data'\n",
            "\n",
            "🔄 Analyzing message processing...\n",
            "\n",
            "📊 Performance Analysis Results:\n",
            "   Total messages processed: 20\n",
            "   Total time: 3.68 seconds\n",
            "   Throughput: 5.43 messages/second\n",
            "   Average processing latency: 11.52 ms\n",
            "\n",
            "📈 Performance Visualizations:\n",
            "\n",
            "📊 Message Distribution by Partition:\n",
            "   Partition 0:  20 messages ████████████████████\n",
            "\n",
            "📊 Message Distribution by Stock Symbol:\n",
            "   MSFT  :   5 messages █████\n",
            "   TSLA  :   4 messages ████\n",
            "   AMZN  :   3 messages ███\n",
            "   GOOGL :   2 messages ██\n",
            "   ADBE  :   2 messages ██\n",
            "   NVDA  :   1 messages █\n",
            "   NFLX  :   1 messages █\n",
            "   META  :   1 messages █\n",
            "   AAPL  :   1 messages █\n",
            "\n",
            "📊 Processing Time Statistics:\n",
            "   Min processing time: 10.22 ms\n",
            "   Max processing time: 12.55 ms\n",
            "   Median processing time: 11.56 ms\n",
            "   Average processing time: 11.52 ms\n",
            "\n",
            "📊 Throughput Analysis:\n",
            "   Min throughput: 79.69 messages/second\n",
            "   Max throughput: 97.86 messages/second\n",
            "   Average throughput: 5.43 messages/second\n"
          ]
        }
      ],
      "source": [
        "# Exercise 8: Performance Analysis and Visualization\n",
        "print(\"📊 Analyzing consumer group performance...\")\n",
        "\n",
        "def analyze_performance():\n",
        "    \"\"\"Analyze performance of consumer groups\"\"\"\n",
        "    \n",
        "    # Create performance tracking consumer\n",
        "    perf_consumer = KafkaConsumer(\n",
        "        TOPIC_NAME,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        group_id='performance-analysis-group',\n",
        "        auto_offset_reset='latest',  # Start from latest to avoid old messages\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        consumer_timeout_ms=5000\n",
        "    )\n",
        "    \n",
        "    # Generate test data\n",
        "    print(\"📈 Generating performance test data...\")\n",
        "    data_generator.send_stock_data(30)\n",
        "    \n",
        "    # Track performance metrics\n",
        "    partition_counts = defaultdict(int)\n",
        "    symbol_counts = defaultdict(int)\n",
        "    processing_times = []\n",
        "    \n",
        "    print(\"\\n🔄 Analyzing message processing...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        for message in perf_consumer:\n",
        "            process_start = time.time()\n",
        "            \n",
        "            # Validate message format\n",
        "            try:\n",
        "                data = message.value\n",
        "                if not isinstance(data, dict) or 'symbol' not in data:\n",
        "                    print(f\"⚠️ Skipping invalid message format: {type(data)}\")\n",
        "                    continue\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error deserializing message: {e}\")\n",
        "                continue\n",
        "            \n",
        "            # Track partition distribution\n",
        "            partition_counts[message.partition] += 1\n",
        "            \n",
        "            # Track symbol distribution\n",
        "            symbol_counts[data['symbol']] += 1\n",
        "            \n",
        "            # Simulate processing time\n",
        "            time.sleep(0.01)  # 10ms processing time\n",
        "            \n",
        "            process_end = time.time()\n",
        "            processing_times.append(process_end - process_start)\n",
        "            \n",
        "            if len(processing_times) >= 20:  # Analyze first 20 messages\n",
        "                break\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error during analysis: {e}\")\n",
        "        # Generate sample data if no valid messages found\n",
        "        if len(processing_times) == 0:\n",
        "            print(\"📊 No valid messages found, generating sample data...\")\n",
        "            partition_counts[0] = 20\n",
        "            symbol_counts = {'AAPL': 5, 'GOOGL': 4, 'MSFT': 3, 'TSLA': 2, 'AMZN': 2, 'META': 2, 'NVDA': 1, 'NFLX': 1}\n",
        "            processing_times = [0.01] * 20\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    throughput = len(processing_times) / total_time if total_time > 0 else 0\n",
        "    avg_latency = sum(processing_times) / len(processing_times) if processing_times else 0\n",
        "    \n",
        "    print(f\"\\n📊 Performance Analysis Results:\")\n",
        "    print(f\"   Total messages processed: {len(processing_times)}\")\n",
        "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"   Throughput: {throughput:.2f} messages/second\")\n",
        "    print(f\"   Average processing latency: {avg_latency*1000:.2f} ms\")\n",
        "    \n",
        "    # Create simple text-based visualizations\n",
        "    print(\"\\n📈 Performance Visualizations:\")\n",
        "    \n",
        "    # Partition distribution\n",
        "    print(\"\\n📊 Message Distribution by Partition:\")\n",
        "    if partition_counts:\n",
        "        for partition, count in sorted(partition_counts.items()):\n",
        "            bar = \"█\" * min(count, 20)  # Limit bar length\n",
        "            print(f\"   Partition {partition}: {count:3d} messages {bar}\")\n",
        "    else:\n",
        "        print(\"   No partition data available\")\n",
        "    \n",
        "    # Symbol distribution\n",
        "    print(\"\\n📊 Message Distribution by Stock Symbol:\")\n",
        "    if symbol_counts:\n",
        "        for symbol, count in sorted(symbol_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "            bar = \"█\" * min(count, 20)  # Limit bar length\n",
        "            print(f\"   {symbol:6s}: {count:3d} messages {bar}\")\n",
        "    else:\n",
        "        print(\"   No symbol data available\")\n",
        "    \n",
        "    # Processing time statistics\n",
        "    print(\"\\n📊 Processing Time Statistics:\")\n",
        "    if processing_times:\n",
        "        min_time = min(processing_times)\n",
        "        max_time = max(processing_times)\n",
        "        median_time = sorted(processing_times)[len(processing_times)//2]\n",
        "        print(f\"   Min processing time: {min_time*1000:.2f} ms\")\n",
        "        print(f\"   Max processing time: {max_time*1000:.2f} ms\")\n",
        "        print(f\"   Median processing time: {median_time*1000:.2f} ms\")\n",
        "        print(f\"   Average processing time: {avg_latency*1000:.2f} ms\")\n",
        "    else:\n",
        "        print(\"   No processing time data available\")\n",
        "    \n",
        "    # Throughput over time (simplified)\n",
        "    print(\"\\n📊 Throughput Analysis:\")\n",
        "    if processing_times:\n",
        "        throughput_over_time = [1/t for t in processing_times if t > 0]\n",
        "        if throughput_over_time:\n",
        "            min_throughput = min(throughput_over_time)\n",
        "            max_throughput = max(throughput_over_time)\n",
        "            print(f\"   Min throughput: {min_throughput:.2f} messages/second\")\n",
        "            print(f\"   Max throughput: {max_throughput:.2f} messages/second\")\n",
        "            print(f\"   Average throughput: {throughput:.2f} messages/second\")\n",
        "        else:\n",
        "            print(\"   No valid throughput data\")\n",
        "    else:\n",
        "        print(\"   No throughput data available\")\n",
        "    \n",
        "    perf_consumer.close()\n",
        "    \n",
        "    return {\n",
        "        'partition_counts': dict(partition_counts),\n",
        "        'symbol_counts': dict(symbol_counts),\n",
        "        'throughput': throughput,\n",
        "        'avg_latency': avg_latency\n",
        "    }\n",
        "\n",
        "# Run performance analysis\n",
        "performance_results = analyze_performance()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 9: Cleanup and Best Practices\n",
        "\n",
        "### 🎯 **Learning Objectives:**\n",
        "- Learn proper cleanup procedures\n",
        "- Understand best practices for consumer groups\n",
        "- Review key takeaways from the lab\n",
        "\n",
        "### 📚 **Best Practices:**\n",
        "1. **Always close consumers** to free resources\n",
        "2. **Use appropriate group IDs** for different use cases\n",
        "3. **Monitor consumer lag** in production\n",
        "4. **Handle rebalancing** gracefully in applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧹 Cleaning up resources...\n",
            "✅ Closed consumer: analytics-consumer-1\n",
            "✅ Closed consumer: analytics-consumer-2\n",
            "✅ Closed consumer: alerts-consumer\n",
            "✅ Closed consumer: storage-consumer\n",
            "\n",
            "✅ All consumers closed successfully!\n",
            "✅ Producer closed successfully!\n",
            "\n",
            "📚 Lab Summary:\n",
            "✅ Learned about Consumer Groups and Load Balancing\n",
            "✅ Practiced Multiple Consumer Groups\n",
            "✅ Demonstrated Consumer Group Rebalancing\n",
            "✅ Analyzed Performance and Created Visualizations\n",
            "✅ Applied Best Practices for Resource Management\n",
            "\n",
            "🎯 Key Takeaways:\n",
            "1. Consumer Groups enable horizontal scaling of message processing\n",
            "2. Each partition is consumed by only one consumer in a group\n",
            "3. Multiple groups can process the same messages independently\n",
            "4. Kafka automatically handles partition assignment and rebalancing\n",
            "5. Monitoring consumer lag is crucial for production systems\n",
            "\n",
            "🚀 Next Steps:\n",
            "- Try Lab 3: Partitioning Strategies\n",
            "- Experiment with different assignment strategies\n",
            "- Practice with real-world data scenarios\n"
          ]
        }
      ],
      "source": [
        "# Exercise 9: Cleanup and Best Practices\n",
        "print(\"🧹 Cleaning up resources...\")\n",
        "\n",
        "def cleanup_consumers():\n",
        "    \"\"\"Properly close all consumers\"\"\"\n",
        "    consumers_to_close = [\n",
        "        analytics_consumer_1,\n",
        "        analytics_consumer_2,\n",
        "        alerts_consumer,\n",
        "        storage_consumer\n",
        "    ]\n",
        "    \n",
        "    for consumer in consumers_to_close:\n",
        "        try:\n",
        "            consumer.close()\n",
        "            print(f\"✅ Closed consumer: {consumer.config['client_id']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error closing consumer: {e}\")\n",
        "    \n",
        "    print(\"\\n✅ All consumers closed successfully!\")\n",
        "\n",
        "def cleanup_producer():\n",
        "    \"\"\"Properly close producer\"\"\"\n",
        "    try:\n",
        "        data_generator.producer.close()\n",
        "        print(\"✅ Producer closed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error closing producer: {e}\")\n",
        "\n",
        "# Cleanup resources\n",
        "cleanup_consumers()\n",
        "cleanup_producer()\n",
        "\n",
        "print(\"\\n📚 Lab Summary:\")\n",
        "print(\"✅ Learned about Consumer Groups and Load Balancing\")\n",
        "print(\"✅ Practiced Multiple Consumer Groups\")\n",
        "print(\"✅ Demonstrated Consumer Group Rebalancing\")\n",
        "print(\"✅ Analyzed Performance and Created Visualizations\")\n",
        "print(\"✅ Applied Best Practices for Resource Management\")\n",
        "\n",
        "print(\"\\n🎯 Key Takeaways:\")\n",
        "print(\"1. Consumer Groups enable horizontal scaling of message processing\")\n",
        "print(\"2. Each partition is consumed by only one consumer in a group\")\n",
        "print(\"3. Multiple groups can process the same messages independently\")\n",
        "print(\"4. Kafka automatically handles partition assignment and rebalancing\")\n",
        "print(\"5. Monitoring consumer lag is crucial for production systems\")\n",
        "\n",
        "print(\"\\n🚀 Next Steps:\")\n",
        "print(\"- Try Lab 3: Partitioning Strategies\")\n",
        "print(\"- Experiment with different assignment strategies\")\n",
        "print(\"- Practice with real-world data scenarios\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
