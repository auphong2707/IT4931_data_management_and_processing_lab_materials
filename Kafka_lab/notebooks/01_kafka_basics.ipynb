{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: Kafka Basics - Stock Market Data Streaming\n",
        "\n",
        "## üéØ Objectives\n",
        "- Understand Kafka fundamentals: Topics, Partitions, Producers, Consumers\n",
        "- Learn message serialization with JSON and custom serializers\n",
        "- Practice basic producer/consumer patterns\n",
        "- Explore Kafka UI for monitoring\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Kafka cluster running (`docker compose up -d`)\n",
        "- Python dependencies installed (`./setup_kafka_lab.sh`)\n",
        "- Basic understanding of message queues\n",
        "\n",
        "## üèóÔ∏è Architecture Overview\n",
        "```\n",
        "Stock Data Generator ‚Üí Kafka Producer ‚Üí Stock Data Topic\n",
        "                                                      ‚Üì\n",
        "                                              Multiple Consumers\n",
        "                                                      ‚Üì\n",
        "                                            Analytics, Alerts, Storage\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kafka-python in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.2.15)\n",
            "Requirement already satisfied: confluent-kafka in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.11.1)\n",
            "Requirement already satisfied: pandas in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (3.8.2)\n",
            "Requirement already satisfied: seaborn in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (0.13.0)\n",
            "Requirement already satisfied: plotly in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (5.17.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Dependencies installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install kafka-python confluent-kafka pandas matplotlib seaborn plotly\n",
        "\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Configured for 10 stock symbols\n",
            "üîó Kafka Bootstrap Servers: localhost:9092\n",
            "üìù Topic Name: stock-data\n"
          ]
        }
      ],
      "source": [
        "# Kafka Configuration\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "TOPIC_NAME = 'stock-data'\n",
        "\n",
        "# Stock symbols for our lab\n",
        "STOCK_SYMBOLS = [\n",
        "    \"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\", \"AMZN\", \n",
        "    \"META\", \"NVDA\", \"NFLX\", \"ADBE\", \"CRM\"\n",
        "]\n",
        "\n",
        "# Base prices for realistic data generation\n",
        "BASE_PRICES = {\n",
        "    \"AAPL\": 150.0, \"GOOGL\": 2800.0, \"MSFT\": 350.0, \"TSLA\": 250.0, \"AMZN\": 3200.0,\n",
        "    \"META\": 300.0, \"NVDA\": 450.0, \"NFLX\": 400.0, \"ADBE\": 500.0, \"CRM\": 200.0\n",
        "}\n",
        "\n",
        "print(f\"üìä Configured for {len(STOCK_SYMBOLS)} stock symbols\")\n",
        "print(f\"üîó Kafka Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "print(f\"üìù Topic Name: {TOPIC_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Stock Data Generator initialized\n",
            "üìä Available symbols: AAPL, GOOGL, MSFT, TSLA, AMZN, META, NVDA, NFLX, ADBE, CRM\n"
          ]
        }
      ],
      "source": [
        "# Stock Data Generator\n",
        "class StockDataGenerator:\n",
        "    \"\"\"Generate realistic OHLCV stock data\"\"\"\n",
        "    \n",
        "    def __init__(self, symbols: List[str], base_prices: Dict[str, float]):\n",
        "        self.symbols = symbols\n",
        "        self.base_prices = base_prices\n",
        "        self.current_prices = base_prices.copy()\n",
        "    \n",
        "    def generate_ohlcv(self, symbol: str) -> Dict:\n",
        "        \"\"\"Generate OHLCV data for a stock symbol\"\"\"\n",
        "        current_price = self.current_prices[symbol]\n",
        "        price_change = random.uniform(-0.01, 0.01)\n",
        "        new_price = current_price * (1 + price_change)\n",
        "        \n",
        "        open_price = round(new_price * random.uniform(0.999, 1.001), 2)\n",
        "        close_price = round(new_price * random.uniform(0.999, 1.001), 2)\n",
        "        high_price = round(max(open_price, close_price) * random.uniform(1.001, 1.003), 2)\n",
        "        low_price = round(min(open_price, close_price) * random.uniform(0.997, 0.999), 2)\n",
        "        \n",
        "        base_volume = random.randint(100000, 1000000)\n",
        "        volume = base_volume + random.randint(-100000, 100000)\n",
        "        \n",
        "        self.current_prices[symbol] = close_price\n",
        "        \n",
        "        return {\n",
        "            \"symbol\": symbol,\n",
        "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"open\": open_price,\n",
        "            \"high\": high_price,\n",
        "            \"low\": low_price,\n",
        "            \"close\": close_price,\n",
        "            \"volume\": volume,\n",
        "            \"exchange\": \"NASDAQ\"\n",
        "        }\n",
        "\n",
        "# Initialize data generator\n",
        "data_generator = StockDataGenerator(STOCK_SYMBOLS, BASE_PRICES)\n",
        "print(\"‚úÖ Stock Data Generator initialized\")\n",
        "print(f\"üìä Available symbols: {', '.join(STOCK_SYMBOLS)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Understanding Kafka Fundamentals\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand Kafka core concepts: Topics, Partitions, Producers, Consumers\n",
        "- Learn about message serialization and deserialization\n",
        "- Practice basic producer/consumer patterns\n",
        "- Explore Kafka cluster monitoring\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Topic**: A category or feed name to which messages are published\n",
        "2. **Partition**: Topics are split into partitions for scalability\n",
        "3. **Producer**: Application that sends messages to Kafka topics\n",
        "4. **Consumer**: Application that reads messages from Kafka topics\n",
        "5. **Broker**: Kafka server that stores and serves messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating Kafka Producer...\n",
            "‚úÖ Kafka Producer created successfully!\n",
            "üì° Bootstrap servers: localhost:9092\n",
            "üìù Topic: stock-data\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Create Kafka Producer\n",
        "print(\"üîß Creating Kafka Producer...\")\n",
        "\n",
        "try:\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "        key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
        "        acks='all',\n",
        "        retries=3,\n",
        "        batch_size=16384,\n",
        "        linger_ms=10,\n",
        "        compression_type='gzip'\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Kafka Producer created successfully!\")\n",
        "    print(f\"üì° Bootstrap servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "    print(f\"üìù Topic: {TOPIC_NAME}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to create producer: {e}\")\n",
        "    print(\"üí° Make sure Kafka cluster is running: docker compose up -d\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Generating and sending stock data...\n",
            "üìä Sending 15 stock data messages...\n",
            "üìä Sent NVDA: $448.27 -> Partition 0\n",
            "üìä Sent AAPL: $150.96 -> Partition 0\n",
            "üìä Sent AMZN: $3178.95 -> Partition 0\n",
            "üìä Sent NFLX: $401.72 -> Partition 0\n",
            "üìä Sent CRM: $198.56 -> Partition 0\n",
            "üìä Sent NFLX: $399.14 -> Partition 0\n",
            "üìä Sent CRM: $200.73 -> Partition 0\n",
            "üìä Sent NVDA: $448.96 -> Partition 0\n",
            "üìä Sent MSFT: $352.86 -> Partition 0\n",
            "üìä Sent GOOGL: $2822.84 -> Partition 0\n",
            "üìä Sent AMZN: $3160.33 -> Partition 0\n",
            "üìä Sent CRM: $202.67 -> Partition 0\n",
            "üìä Sent NVDA: $451.44 -> Partition 0\n",
            "üìä Sent NVDA: $450.19 -> Partition 0\n",
            "üìä Sent TSLA: $249.75 -> Partition 0\n",
            "‚úÖ Successfully sent 15 messages to topic 'stock-data'\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: Generate and Send Stock Data\n",
        "print(\"üìà Generating and sending stock data...\")\n",
        "\n",
        "def send_stock_data(num_messages: int = 10):\n",
        "    \"\"\"Send stock data to Kafka topic\"\"\"\n",
        "    print(f\"üìä Sending {num_messages} stock data messages...\")\n",
        "    \n",
        "    for i in range(num_messages):\n",
        "        symbol = random.choice(STOCK_SYMBOLS)\n",
        "        ohlcv_data = data_generator.generate_ohlcv(symbol)\n",
        "        \n",
        "        future = producer.send(TOPIC_NAME, key=symbol, value=ohlcv_data)\n",
        "        record_metadata = future.get(timeout=10)\n",
        "        \n",
        "        print(f\"üìä Sent {symbol}: ${ohlcv_data['close']} -> Partition {record_metadata.partition}\")\n",
        "        time.sleep(0.1)\n",
        "    \n",
        "    producer.flush()\n",
        "    print(f\"‚úÖ Successfully sent {num_messages} messages to topic '{TOPIC_NAME}'\")\n",
        "\n",
        "# Send some test data\n",
        "send_stock_data(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating Kafka Consumer...\n",
            "‚úÖ Kafka Consumer created successfully!\n",
            "üë• Consumer Group: stock-analytics-group\n",
            "üìù Topic: stock-data\n",
            "üîÑ Auto Offset Reset: earliest\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: Create Kafka Consumer\n",
        "print(\"üîß Creating Kafka Consumer...\")\n",
        "\n",
        "try:\n",
        "    consumer = KafkaConsumer(\n",
        "        TOPIC_NAME,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        group_id='stock-analytics-group',\n",
        "        auto_offset_reset='earliest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
        "        consumer_timeout_ms=5000\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Kafka Consumer created successfully!\")\n",
        "    print(f\"üë• Consumer Group: stock-analytics-group\")\n",
        "    print(f\"üìù Topic: {TOPIC_NAME}\")\n",
        "    print(f\"üîÑ Auto Offset Reset: earliest\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to create consumer: {e}\")\n",
        "    print(\"üí° Make sure Kafka cluster is running: docker compose up -d\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Consuming messages from Kafka...\n",
            "üöÄ Starting to consume up to 10 messages...\n",
            "üìä Message 1:\n",
            "   Key: AAPL\n",
            "   Symbol: AAPL\n",
            "   Price: $147.29\n",
            "   Volume: 849,059\n",
            "   Partition: 0\n",
            "   Offset: 0\n",
            "   Timestamp: 2025-09-21 22:56:47.238000\n",
            "\n",
            "üìä Message 2:\n",
            "   Key: CRM\n",
            "   Symbol: CRM\n",
            "   Price: $200.24\n",
            "   Volume: 320,535\n",
            "   Partition: 0\n",
            "   Offset: 1\n",
            "   Timestamp: 2025-09-21 23:04:43.846000\n",
            "\n",
            "üìä Message 3:\n",
            "   Key: MSFT\n",
            "   Symbol: MSFT\n",
            "   Price: $353.31\n",
            "   Volume: 613,630\n",
            "   Partition: 0\n",
            "   Offset: 2\n",
            "   Timestamp: 2025-09-21 23:04:43.977000\n",
            "\n",
            "üìä Message 4:\n",
            "   Key: AMZN\n",
            "   Symbol: AMZN\n",
            "   Price: $3193.57\n",
            "   Volume: 1,016,545\n",
            "   Partition: 0\n",
            "   Offset: 3\n",
            "   Timestamp: 2025-09-21 23:04:44.094000\n",
            "\n",
            "üìä Message 5:\n",
            "   Key: META\n",
            "   Symbol: META\n",
            "   Price: $300.17\n",
            "   Volume: 692,115\n",
            "   Partition: 0\n",
            "   Offset: 4\n",
            "   Timestamp: 2025-09-21 23:04:44.215000\n",
            "\n",
            "üìä Message 6:\n",
            "   Key: TSLA\n",
            "   Symbol: TSLA\n",
            "   Price: $249.58\n",
            "   Volume: 881,526\n",
            "   Partition: 0\n",
            "   Offset: 5\n",
            "   Timestamp: 2025-09-21 23:04:44.337000\n",
            "\n",
            "üìä Message 7:\n",
            "   Key: NVDA\n",
            "   Symbol: NVDA\n",
            "   Price: $453.01\n",
            "   Volume: 303,078\n",
            "   Partition: 0\n",
            "   Offset: 6\n",
            "   Timestamp: 2025-09-21 23:04:44.461000\n",
            "\n",
            "üìä Message 8:\n",
            "   Key: NFLX\n",
            "   Symbol: NFLX\n",
            "   Price: $401.11\n",
            "   Volume: 107,421\n",
            "   Partition: 0\n",
            "   Offset: 7\n",
            "   Timestamp: 2025-09-21 23:04:44.584000\n",
            "\n",
            "üìä Message 9:\n",
            "   Key: TSLA\n",
            "   Symbol: TSLA\n",
            "   Price: $248.7\n",
            "   Volume: 590,190\n",
            "   Partition: 0\n",
            "   Offset: 8\n",
            "   Timestamp: 2025-09-21 23:04:44.707000\n",
            "\n",
            "üìä Message 10:\n",
            "   Key: NFLX\n",
            "   Symbol: NFLX\n",
            "   Price: $401.48\n",
            "   Volume: 803,814\n",
            "   Partition: 0\n",
            "   Offset: 9\n",
            "   Timestamp: 2025-09-21 23:04:44.827000\n",
            "\n",
            "üìà Consumption Summary:\n",
            "   Total messages consumed: 10\n",
            "   Partition distribution: {0: 10}\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: Consume Messages\n",
        "print(\"üîÑ Consuming messages from Kafka...\")\n",
        "\n",
        "def consume_messages(max_messages: int = 10):\n",
        "    \"\"\"Consume messages from Kafka topic\"\"\"\n",
        "    messages_consumed = 0\n",
        "    partition_counts = {}\n",
        "    \n",
        "    print(f\"üöÄ Starting to consume up to {max_messages} messages...\")\n",
        "    \n",
        "    try:\n",
        "        for message in consumer:\n",
        "            if messages_consumed >= max_messages:\n",
        "                break\n",
        "            \n",
        "            partition = message.partition\n",
        "            partition_counts[partition] = partition_counts.get(partition, 0) + 1\n",
        "            \n",
        "            key = message.key\n",
        "            value = message.value\n",
        "            offset = message.offset\n",
        "            timestamp = message.timestamp\n",
        "            \n",
        "            print(f\"üìä Message {messages_consumed + 1}:\")\n",
        "            print(f\"   Key: {key}\")\n",
        "            print(f\"   Symbol: {value['symbol']}\")\n",
        "            print(f\"   Price: ${value['close']}\")\n",
        "            print(f\"   Volume: {value['volume']:,}\")\n",
        "            print(f\"   Partition: {partition}\")\n",
        "            print(f\"   Offset: {offset}\")\n",
        "            print(f\"   Timestamp: {datetime.fromtimestamp(timestamp/1000)}\")\n",
        "            print()\n",
        "            \n",
        "            messages_consumed += 1\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Finished consuming: {e}\")\n",
        "    \n",
        "    print(f\"üìà Consumption Summary:\")\n",
        "    print(f\"   Total messages consumed: {messages_consumed}\")\n",
        "    print(f\"   Partition distribution: {partition_counts}\")\n",
        "    \n",
        "    return messages_consumed, partition_counts\n",
        "\n",
        "# Consume messages\n",
        "messages_count, partition_distribution = consume_messages(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Demonstrating custom serialization...\n",
            "‚úÖ Custom serializers/deserializers created!\n",
            "\n",
            "üß™ Testing custom serialization...\n",
            "üìä Sent complex data to partition 0\n",
            "\n",
            "üîÑ Consuming complex data...\n",
            "‚úÖ Custom serialization test completed!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 5: Custom Serializers and Deserializers\n",
        "print(\"üîß Demonstrating custom serialization...\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "class CustomSerializer:\n",
        "    \"\"\"Custom serializer for complex objects\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def serialize(obj):\n",
        "        \"\"\"Serialize object to bytes\"\"\"\n",
        "        return pickle.dumps(obj)\n",
        "    \n",
        "    @staticmethod\n",
        "    def deserialize(data):\n",
        "        \"\"\"Deserialize bytes to object\"\"\"\n",
        "        return pickle.loads(data)\n",
        "\n",
        "# Create producer with custom serializer\n",
        "custom_producer = KafkaProducer(\n",
        "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "    value_serializer=CustomSerializer.serialize,\n",
        "    key_serializer=lambda k: k.encode('utf-8') if k else None\n",
        ")\n",
        "\n",
        "# Create consumer with custom deserializer\n",
        "custom_consumer = KafkaConsumer(\n",
        "    TOPIC_NAME,\n",
        "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "    group_id='custom-serialization-group',\n",
        "    auto_offset_reset='latest',\n",
        "    enable_auto_commit=True,\n",
        "    value_deserializer=CustomSerializer.deserialize,\n",
        "    key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
        "    consumer_timeout_ms=3000\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Custom serializers/deserializers created!\")\n",
        "\n",
        "# Test custom serialization\n",
        "print(\"\\nüß™ Testing custom serialization...\")\n",
        "\n",
        "# Create a complex object\n",
        "complex_data = {\n",
        "    \"symbol\": \"AAPL\",\n",
        "    \"price_data\": {\n",
        "        \"open\": 150.0,\n",
        "        \"high\": 155.0,\n",
        "        \"low\": 148.0,\n",
        "        \"close\": 152.0\n",
        "    },\n",
        "    \"indicators\": {\n",
        "        \"sma_20\": 150.5,\n",
        "        \"rsi\": 65.2,\n",
        "        \"macd\": 1.2\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"source\": \"market_data_api\",\n",
        "        \"version\": \"1.0\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Send complex data\n",
        "future = custom_producer.send(TOPIC_NAME, key=\"AAPL\", value=complex_data)\n",
        "record_metadata = future.get(timeout=10)\n",
        "\n",
        "print(f\"üìä Sent complex data to partition {record_metadata.partition}\")\n",
        "\n",
        "# Consume and verify\n",
        "print(\"\\nüîÑ Consuming complex data...\")\n",
        "for message in custom_consumer:\n",
        "    print(f\"üìä Received complex data:\")\n",
        "    print(f\"   Symbol: {message.value['symbol']}\")\n",
        "    print(f\"   Price: ${message.value['price_data']['close']}\")\n",
        "    print(f\"   RSI: {message.value['indicators']['rsi']}\")\n",
        "    print(f\"   Source: {message.value['metadata']['source']}\")\n",
        "    break\n",
        "\n",
        "print(\"‚úÖ Custom serialization test completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ°Ô∏è Demonstrating error handling and retry logic...\n",
            "\n",
            "üß™ Testing error handling...\n",
            "‚úÖ Message sent successfully to partition 0\n",
            "‚úÖ Message sent successfully to partition 0\n",
            "‚úÖ Message sent successfully to partition 0\n",
            "‚úÖ Message sent successfully to partition 0\n",
            "‚úÖ Message sent successfully to partition 0\n",
            "üîÑ Starting robust message consumption...\n",
            "\n",
            "üìà Processing Summary:\n",
            "   Messages processed: 0\n",
            "   Errors handled: 0\n",
            "\n",
            "‚úÖ Error handling demonstration completed!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 6: Error Handling and Retry Logic\n",
        "print(\"üõ°Ô∏è Demonstrating error handling and retry logic...\")\n",
        "\n",
        "from functools import wraps\n",
        "\n",
        "class RetryableError(Exception):\n",
        "    \"\"\"Custom exception for retryable errors\"\"\"\n",
        "    pass\n",
        "\n",
        "class NonRetryableError(Exception):\n",
        "    \"\"\"Custom exception for non-retryable errors\"\"\"\n",
        "    pass\n",
        "\n",
        "def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):\n",
        "    \"\"\"Decorator for retry logic with exponential backoff\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            delay = base_delay\n",
        "            for attempt in range(max_retries + 1):\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except RetryableError as e:\n",
        "                    if attempt == max_retries:\n",
        "                        print(f\"‚ùå Max retries ({max_retries}) exceeded. Giving up.\")\n",
        "                        raise e\n",
        "                    \n",
        "                    print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}\")\n",
        "                    print(f\"üîÑ Retrying in {delay} seconds...\")\n",
        "                    time.sleep(delay)\n",
        "                    delay = min(delay * 2, max_delay)\n",
        "                    \n",
        "                except NonRetryableError as e:\n",
        "                    print(f\"‚ùå Non-retryable error: {e}\")\n",
        "                    raise e\n",
        "                    \n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Robust producer with error handling\n",
        "@retry_with_backoff(max_retries=3, base_delay=1)\n",
        "def send_message_with_retry(producer, topic, key, value):\n",
        "    \"\"\"Send message with retry logic\"\"\"\n",
        "    try:\n",
        "        future = producer.send(topic, key=key, value=value)\n",
        "        record_metadata = future.get(timeout=10)\n",
        "        print(f\"‚úÖ Message sent successfully to partition {record_metadata.partition}\")\n",
        "        return record_metadata\n",
        "    except Exception as e:\n",
        "        if \"timeout\" in str(e).lower():\n",
        "            raise RetryableError(f\"Timeout sending message: {e}\")\n",
        "        else:\n",
        "            raise NonRetryableError(f\"Non-retryable error: {e}\")\n",
        "\n",
        "# Create dedicated consumer for error handling demo\n",
        "error_handling_consumer = KafkaConsumer(\n",
        "    TOPIC_NAME,\n",
        "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "    group_id='error-handling-demo-group',\n",
        "    auto_offset_reset='latest',\n",
        "    enable_auto_commit=True,\n",
        "    value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "    key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
        "    consumer_timeout_ms=3000\n",
        ")\n",
        "\n",
        "# Robust consumer with error handling\n",
        "def consume_with_error_handling(consumer, max_messages=5):\n",
        "    \"\"\"Consume messages with error handling\"\"\"\n",
        "    messages_processed = 0\n",
        "    errors_handled = 0\n",
        "    \n",
        "    print(f\"üîÑ Starting robust message consumption...\")\n",
        "    \n",
        "    try:\n",
        "        for message in consumer:\n",
        "            if messages_processed >= max_messages:\n",
        "                break\n",
        "                \n",
        "            try:\n",
        "                data = message.value\n",
        "                \n",
        "                if not isinstance(data, dict) or 'symbol' not in data:\n",
        "                    print(f\"‚ö†Ô∏è Invalid message format, skipping...\")\n",
        "                    errors_handled += 1\n",
        "                    continue\n",
        "                \n",
        "                if random.random() < 0.3:  # 30% chance of error\n",
        "                    raise RetryableError(\"Simulated processing error\")\n",
        "                \n",
        "                print(f\"üìä Processed message: {data['symbol']} - ${data['close']}\")\n",
        "                messages_processed += 1\n",
        "                \n",
        "            except RetryableError as e:\n",
        "                print(f\"‚ö†Ô∏è Retryable error processing message: {e}\")\n",
        "                errors_handled += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Non-retryable error: {e}\")\n",
        "                errors_handled += 1\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Consumer error: {e}\")\n",
        "    \n",
        "    print(f\"\\nüìà Processing Summary:\")\n",
        "    print(f\"   Messages processed: {messages_processed}\")\n",
        "    print(f\"   Errors handled: {errors_handled}\")\n",
        "    \n",
        "    return messages_processed, errors_handled\n",
        "\n",
        "# Test error handling\n",
        "print(\"\\nüß™ Testing error handling...\")\n",
        "\n",
        "# Send some test data\n",
        "for i in range(5):\n",
        "    symbol = random.choice(STOCK_SYMBOLS)\n",
        "    ohlcv_data = data_generator.generate_ohlcv(symbol)\n",
        "    \n",
        "    try:\n",
        "        send_message_with_retry(producer, TOPIC_NAME, symbol, ohlcv_data)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to send message after retries: {e}\")\n",
        "\n",
        "# Test consumer error handling\n",
        "messages_processed, errors_handled = consume_with_error_handling(error_handling_consumer, 5)\n",
        "\n",
        "print(\"\\n‚úÖ Error handling demonstration completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Analyzing Kafka performance...\n",
            "‚úÖ High-performance producer and consumer created!\n",
            "\n",
            "üöÄ Testing producer throughput...\n",
            "üìä Producer Performance:\n",
            "   Messages sent: 100\n",
            "   Time taken: 5.44 seconds\n",
            "   Throughput: 18.40 messages/second\n",
            "\n",
            "üîÑ Testing consumer throughput...\n",
            "üìä Consumer Performance:\n",
            "   Messages consumed: 0\n",
            "   Time taken: 2.00 seconds\n",
            "   Throughput: 0.00 messages/second\n",
            "\n",
            "üìà Performance Summary:\n",
            "   avg_producer_throughput: 18.40\n",
            "   max_producer_throughput: 18.40\n",
            "   avg_consumer_throughput: 0.00\n",
            "   max_consumer_throughput: 0.00\n",
            "   avg_message_size_bytes: 166.35\n",
            "   total_messages: 100\n",
            "   partition_distribution: {0: 100}\n",
            "\n",
            "‚úÖ Performance analysis completed!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 7: Performance Analysis and Monitoring\n",
        "print(\"üìä Analyzing Kafka performance...\")\n",
        "\n",
        "from collections import defaultdict\n",
        "import statistics\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitor Kafka performance metrics\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            'producer_throughput': [],\n",
        "            'consumer_throughput': [],\n",
        "            'latency': [],\n",
        "            'message_sizes': [],\n",
        "            'partition_distribution': defaultdict(int)\n",
        "        }\n",
        "    \n",
        "    def record_producer_metric(self, messages_sent, time_taken):\n",
        "        \"\"\"Record producer performance metric\"\"\"\n",
        "        throughput = messages_sent / time_taken if time_taken > 0 else 0\n",
        "        self.metrics['producer_throughput'].append(throughput)\n",
        "    \n",
        "    def record_consumer_metric(self, messages_consumed, time_taken):\n",
        "        \"\"\"Record consumer performance metric\"\"\"\n",
        "        throughput = messages_consumed / time_taken if time_taken > 0 else 0\n",
        "        self.metrics['consumer_throughput'].append(throughput)\n",
        "    \n",
        "    def record_latency(self, latency_ms):\n",
        "        \"\"\"Record message latency\"\"\"\n",
        "        self.metrics['latency'].append(latency_ms)\n",
        "    \n",
        "    def record_message_size(self, size_bytes):\n",
        "        \"\"\"Record message size\"\"\"\n",
        "        self.metrics['message_sizes'].append(size_bytes)\n",
        "    \n",
        "    def record_partition(self, partition):\n",
        "        \"\"\"Record partition distribution\"\"\"\n",
        "        self.metrics['partition_distribution'][partition] += 1\n",
        "    \n",
        "    def get_summary(self):\n",
        "        \"\"\"Get performance summary\"\"\"\n",
        "        summary = {}\n",
        "        \n",
        "        if self.metrics['producer_throughput']:\n",
        "            summary['avg_producer_throughput'] = statistics.mean(self.metrics['producer_throughput'])\n",
        "            summary['max_producer_throughput'] = max(self.metrics['producer_throughput'])\n",
        "        \n",
        "        if self.metrics['consumer_throughput']:\n",
        "            summary['avg_consumer_throughput'] = statistics.mean(self.metrics['consumer_throughput'])\n",
        "            summary['max_consumer_throughput'] = max(self.metrics['consumer_throughput'])\n",
        "        \n",
        "        if self.metrics['latency']:\n",
        "            summary['avg_latency_ms'] = statistics.mean(self.metrics['latency'])\n",
        "            summary['max_latency_ms'] = max(self.metrics['latency'])\n",
        "            summary['min_latency_ms'] = min(self.metrics['latency'])\n",
        "        \n",
        "        if self.metrics['message_sizes']:\n",
        "            summary['avg_message_size_bytes'] = statistics.mean(self.metrics['message_sizes'])\n",
        "            summary['total_messages'] = len(self.metrics['message_sizes'])\n",
        "        \n",
        "        summary['partition_distribution'] = dict(self.metrics['partition_distribution'])\n",
        "        \n",
        "        return summary\n",
        "\n",
        "# Initialize performance monitor\n",
        "perf_monitor = PerformanceMonitor()\n",
        "\n",
        "# High-performance producer with gzip compression (compatible)\n",
        "high_perf_producer = KafkaProducer(\n",
        "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "    key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
        "    batch_size=32768,\n",
        "    linger_ms=50,\n",
        "    compression_type='gzip',  # Use gzip (always available)\n",
        "    acks=1,\n",
        "    retries=0\n",
        ")\n",
        "\n",
        "# High-performance consumer\n",
        "high_perf_consumer = KafkaConsumer(\n",
        "    TOPIC_NAME,\n",
        "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "    group_id='performance-test-group',\n",
        "    auto_offset_reset='latest',\n",
        "    enable_auto_commit=True,\n",
        "    value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "    key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
        "    consumer_timeout_ms=2000,\n",
        "    fetch_min_bytes=1024,\n",
        "    fetch_max_wait_ms=500\n",
        ")\n",
        "\n",
        "print(\"‚úÖ High-performance producer and consumer created!\")\n",
        "\n",
        "# Performance test: Producer throughput\n",
        "print(\"\\nüöÄ Testing producer throughput...\")\n",
        "num_messages = 100\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(num_messages):\n",
        "    symbol = random.choice(STOCK_SYMBOLS)\n",
        "    ohlcv_data = data_generator.generate_ohlcv(symbol)\n",
        "    \n",
        "    message_size = len(json.dumps(ohlcv_data).encode('utf-8'))\n",
        "    perf_monitor.record_message_size(message_size)\n",
        "    \n",
        "    future = high_perf_producer.send(TOPIC_NAME, key=symbol, value=ohlcv_data)\n",
        "    record_metadata = future.get(timeout=5)\n",
        "    \n",
        "    perf_monitor.record_partition(record_metadata.partition)\n",
        "\n",
        "high_perf_producer.flush()\n",
        "end_time = time.time()\n",
        "producer_time = end_time - start_time\n",
        "\n",
        "perf_monitor.record_producer_metric(num_messages, producer_time)\n",
        "\n",
        "print(f\"üìä Producer Performance:\")\n",
        "print(f\"   Messages sent: {num_messages}\")\n",
        "print(f\"   Time taken: {producer_time:.2f} seconds\")\n",
        "print(f\"   Throughput: {num_messages/producer_time:.2f} messages/second\")\n",
        "\n",
        "# Performance test: Consumer throughput\n",
        "print(\"\\nüîÑ Testing consumer throughput...\")\n",
        "start_time = time.time()\n",
        "messages_consumed = 0\n",
        "\n",
        "for message in high_perf_consumer:\n",
        "    if messages_consumed >= num_messages:\n",
        "        break\n",
        "    \n",
        "    data = message.value\n",
        "    latency_ms = (time.time() * 1000) - message.timestamp\n",
        "    perf_monitor.record_latency(latency_ms)\n",
        "    messages_consumed += 1\n",
        "\n",
        "end_time = time.time()\n",
        "consumer_time = end_time - start_time\n",
        "\n",
        "perf_monitor.record_consumer_metric(messages_consumed, consumer_time)\n",
        "\n",
        "print(f\"üìä Consumer Performance:\")\n",
        "print(f\"   Messages consumed: {messages_consumed}\")\n",
        "print(f\"   Time taken: {consumer_time:.2f} seconds\")\n",
        "print(f\"   Throughput: {messages_consumed/consumer_time:.2f} messages/second\")\n",
        "\n",
        "# Get performance summary\n",
        "summary = perf_monitor.get_summary()\n",
        "print(f\"\\nüìà Performance Summary:\")\n",
        "for key, value in summary.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"   {key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"   {key}: {value}\")\n",
        "\n",
        "print(\"\\n‚úÖ Performance analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Cleaning up resources and reviewing best practices...\n",
            "üîß Closing Kafka resources...\n",
            "‚úÖ Closed Producer\n",
            "‚úÖ Closed Custom Producer\n",
            "‚úÖ Closed High Performance Producer\n",
            "‚úÖ Closed Consumer\n",
            "‚úÖ Closed Custom Consumer\n",
            "‚úÖ Closed High Performance Consumer\n",
            "‚úÖ Closed Error Handling Consumer\n",
            "‚úÖ Closed Visualization Consumer\n",
            "\n",
            "‚úÖ All resources closed successfully!\n",
            "\n",
            "üìö Kafka Best Practices Summary:\n",
            "\n",
            "üîß Producer Best Practices:\n",
            "   1. Use appropriate acknowledgment levels (acks)\n",
            "   2. Implement retry logic with exponential backoff\n",
            "   3. Use batching for better throughput\n",
            "   4. Choose appropriate compression (gzip, snappy, lz4)\n",
            "   5. Always call flush() before closing\n",
            "   6. Use meaningful keys for partitioning\n",
            "\n",
            "üîÑ Consumer Best Practices:\n",
            "   1. Use consumer groups for scalability\n",
            "   2. Implement proper error handling\n",
            "   3. Monitor consumer lag\n",
            "   4. Use appropriate auto_offset_reset\n",
            "   5. Implement idempotent processing\n",
            "   6. Handle rebalancing gracefully\n",
            "\n",
            "üìä Performance Best Practices:\n",
            "   1. Tune batch sizes based on use case\n",
            "   2. Use compression for large messages\n",
            "   3. Monitor throughput and latency\n",
            "   4. Optimize partition count\n",
            "   5. Use appropriate replication factor\n",
            "   6. Implement proper monitoring\n",
            "\n",
            "üõ°Ô∏è Error Handling Best Practices:\n",
            "   1. Implement retry logic with backoff\n",
            "   2. Use dead letter queues for failed messages\n",
            "   3. Implement circuit breaker patterns\n",
            "   4. Log errors appropriately\n",
            "   5. Monitor error rates\n",
            "   6. Implement graceful degradation\n",
            "\n",
            "üìà Monitoring Best Practices:\n",
            "   1. Monitor producer/consumer throughput\n",
            "   2. Track message latency\n",
            "   3. Monitor consumer lag\n",
            "   4. Set up alerts for failures\n",
            "   5. Use proper logging levels\n",
            "   6. Implement health checks\n",
            "\n",
            "üéØ Lab 1 Summary - What We Learned:\n",
            "‚úÖ Kafka Fundamentals: Topics, Partitions, Producers, Consumers\n",
            "‚úÖ Message Serialization: JSON and custom serializers\n",
            "‚úÖ Error Handling: Retry logic and exception handling\n",
            "‚úÖ Performance Analysis: Throughput and latency measurement\n",
            "‚úÖ Data Visualization: Real-time charts and dashboards\n",
            "‚úÖ Best Practices: Resource management and optimization\n",
            "\n",
            "üöÄ Next Steps:\n",
            "- Lab 2: Consumer Groups and Load Balancing\n",
            "- Lab 3: Partitioning Strategies\n",
            "- Lab 4: Offset Management\n",
            "- Lab 5: Real-time Analytics\n",
            "\n",
            "üí° Key Takeaways:\n",
            "1. Kafka enables high-throughput, low-latency messaging\n",
            "2. Proper configuration is crucial for performance\n",
            "3. Error handling and monitoring are essential\n",
            "4. Visualization helps understand data patterns\n",
            "5. Resource cleanup prevents memory leaks\n",
            "\n",
            "üéâ Lab 1 completed successfully!\n",
            "Ready to move on to Lab 2: Consumer Groups!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 9: Best Practices and Cleanup\n",
        "print(\"üßπ Cleaning up resources and reviewing best practices...\")\n",
        "\n",
        "def cleanup_resources():\n",
        "    \"\"\"Properly close all Kafka resources\"\"\"\n",
        "    resources_to_close = [\n",
        "        ('Producer', producer),\n",
        "        ('Custom Producer', custom_producer),\n",
        "        ('High Performance Producer', high_perf_producer),\n",
        "        ('Consumer', consumer),\n",
        "        ('Custom Consumer', custom_consumer),\n",
        "        ('High Performance Consumer', high_perf_consumer),\n",
        "        ('Error Handling Consumer', error_handling_consumer),\n",
        "        ('Visualization Consumer', visualization_consumer)\n",
        "    ]\n",
        "    \n",
        "    print(\"üîß Closing Kafka resources...\")\n",
        "    \n",
        "    for resource_name, resource in resources_to_close:\n",
        "        try:\n",
        "            if resource:\n",
        "                resource.close()\n",
        "                print(f\"‚úÖ Closed {resource_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error closing {resource_name}: {e}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ All resources closed successfully!\")\n",
        "\n",
        "# Cleanup all resources\n",
        "cleanup_resources()\n",
        "\n",
        "# Best Practices Summary\n",
        "print(\"\\nüìö Kafka Best Practices Summary:\")\n",
        "print(\"\\nüîß Producer Best Practices:\")\n",
        "print(\"   1. Use appropriate acknowledgment levels (acks)\")\n",
        "print(\"   2. Implement retry logic with exponential backoff\")\n",
        "print(\"   3. Use batching for better throughput\")\n",
        "print(\"   4. Choose appropriate compression (gzip, snappy, lz4)\")\n",
        "print(\"   5. Always call flush() before closing\")\n",
        "print(\"   6. Use meaningful keys for partitioning\")\n",
        "\n",
        "print(\"\\nüîÑ Consumer Best Practices:\")\n",
        "print(\"   1. Use consumer groups for scalability\")\n",
        "print(\"   2. Implement proper error handling\")\n",
        "print(\"   3. Monitor consumer lag\")\n",
        "print(\"   4. Use appropriate auto_offset_reset\")\n",
        "print(\"   5. Implement idempotent processing\")\n",
        "print(\"   6. Handle rebalancing gracefully\")\n",
        "\n",
        "print(\"\\nüìä Performance Best Practices:\")\n",
        "print(\"   1. Tune batch sizes based on use case\")\n",
        "print(\"   2. Use compression for large messages\")\n",
        "print(\"   3. Monitor throughput and latency\")\n",
        "print(\"   4. Optimize partition count\")\n",
        "print(\"   5. Use appropriate replication factor\")\n",
        "print(\"   6. Implement proper monitoring\")\n",
        "\n",
        "print(\"\\nüõ°Ô∏è Error Handling Best Practices:\")\n",
        "print(\"   1. Implement retry logic with backoff\")\n",
        "print(\"   2. Use dead letter queues for failed messages\")\n",
        "print(\"   3. Implement circuit breaker patterns\")\n",
        "print(\"   4. Log errors appropriately\")\n",
        "print(\"   5. Monitor error rates\")\n",
        "print(\"   6. Implement graceful degradation\")\n",
        "\n",
        "print(\"\\nüìà Monitoring Best Practices:\")\n",
        "print(\"   1. Monitor producer/consumer throughput\")\n",
        "print(\"   2. Track message latency\")\n",
        "print(\"   3. Monitor consumer lag\")\n",
        "print(\"   4. Set up alerts for failures\")\n",
        "print(\"   5. Use proper logging levels\")\n",
        "print(\"   6. Implement health checks\")\n",
        "\n",
        "# Lab Summary\n",
        "print(\"\\nüéØ Lab 1 Summary - What We Learned:\")\n",
        "print(\"‚úÖ Kafka Fundamentals: Topics, Partitions, Producers, Consumers\")\n",
        "print(\"‚úÖ Message Serialization: JSON and custom serializers\")\n",
        "print(\"‚úÖ Error Handling: Retry logic and exception handling\")\n",
        "print(\"‚úÖ Performance Analysis: Throughput and latency measurement\")\n",
        "print(\"‚úÖ Data Visualization: Real-time charts and dashboards\")\n",
        "print(\"‚úÖ Best Practices: Resource management and optimization\")\n",
        "\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"- Lab 2: Consumer Groups and Load Balancing\")\n",
        "print(\"- Lab 3: Partitioning Strategies\")\n",
        "print(\"- Lab 4: Offset Management\")\n",
        "print(\"- Lab 5: Real-time Analytics\")\n",
        "\n",
        "print(\"\\nüí° Key Takeaways:\")\n",
        "print(\"1. Kafka enables high-throughput, low-latency messaging\")\n",
        "print(\"2. Proper configuration is crucial for performance\")\n",
        "print(\"3. Error handling and monitoring are essential\")\n",
        "print(\"4. Visualization helps understand data patterns\")\n",
        "print(\"5. Resource cleanup prevents memory leaks\")\n",
        "\n",
        "print(\"\\nüéâ Lab 1 completed successfully!\")\n",
        "print(\"Ready to move on to Lab 2: Consumer Groups!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
