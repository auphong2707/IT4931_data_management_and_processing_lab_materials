{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyIceberg Lab - Apache Iceberg with Python\n",
        "\n",
        "## Lab Objectives\n",
        "In this lab, we will learn how to:\n",
        "1. Install and configure PyIceberg\n",
        "2. Create and manage Iceberg catalogs\n",
        "3. Create Iceberg tables and manipulate data\n",
        "4. Perform schema evolution\n",
        "5. Query and analyze data\n",
        "\n",
        "## What is Apache Iceberg?\n",
        "Apache Iceberg is an open, high-performance table format for huge analytic datasets, bringing reliability and simplicity to big data. PyIceberg is the Python library for programmatic access to Iceberg table metadata and data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install PyIceberg\n",
        "\n",
        "**Note:** If you have already set up the conda environment `datalab` following the instructions in README.md, skip this section and move to the next cell.\n",
        "\n",
        "If not set up yet, you can install directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (25.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pyiceberg[hive,pyarrow,s3fs] in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (0.10.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=5.5 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (6.2.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (8.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.1.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (2025.9.0)\n",
            "Requirement already satisfied: mmh3<6.0.0,>=4.0.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (5.2.0)\n",
            "Requirement already satisfied: pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (2.11.9)\n",
            "Requirement already satisfied: pyparsing<4.0.0,>=3.1.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (3.2.4)\n",
            "Requirement already satisfied: pyroaring<2.0.0,>=1.0.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (2.32.5)\n",
            "Requirement already satisfied: rich<15.0.0,>=10.11.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (14.1.0)\n",
            "Requirement already satisfied: sortedcontainers==2.4.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (2.4.0)\n",
            "Requirement already satisfied: strictyaml<2.0.0,>=1.7.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (1.7.3)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (9.1.2)\n",
            "Requirement already satisfied: pyarrow>=17.0.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (21.0.0)\n",
            "Requirement already satisfied: pyiceberg-core<0.7.0,>=0.5.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (0.6.0)\n",
            "Requirement already satisfied: s3fs>=2023.1.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (2025.9.0)\n",
            "Requirement already satisfied: thrift<1.0.0,>=0.13.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pyiceberg[hive,pyarrow,s3fs]) (0.22.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg[hive,pyarrow,s3fs]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg[hive,pyarrow,s3fs]) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg[hive,pyarrow,s3fs]) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg[hive,pyarrow,s3fs]) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg[hive,pyarrow,s3fs]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg[hive,pyarrow,s3fs]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg[hive,pyarrow,s3fs]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg[hive,pyarrow,s3fs]) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from rich<15.0.0,>=10.11.0->pyiceberg[hive,pyarrow,s3fs]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from rich<15.0.0,>=10.11.0->pyiceberg[hive,pyarrow,s3fs]) (2.19.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from strictyaml<2.0.0,>=1.7.0->pyiceberg[hive,pyarrow,s3fs]) (2.9.0.post0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=10.11.0->pyiceberg[hive,pyarrow,s3fs]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from python-dateutil>=2.6.0->strictyaml<2.0.0,>=1.7.0->pyiceberg[hive,pyarrow,s3fs]) (1.17.0)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (2.24.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (3.12.15)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.40.19,>=1.40.15 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (1.40.18)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (1.0.1)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (6.6.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.1.0->pyiceberg[hive,pyarrow,s3fs]) (1.20.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "zsh:1: unmatched '\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install PyIceberg with necessary dependencies\n",
        "# Only run this cell if you haven't set up the conda environment\n",
        "%pip install --upgrade pip\n",
        "%pip install \"pyiceberg[s3fs,hive,pyarrow]\"\n",
        "%pip install 'pyiceberg[sql-sqlite]\n",
        "% pip install 'pyiceberg[sql-postgres]'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import tempfile\n",
        "from pyiceberg.catalog import load_catalog\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow.compute as pc\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Warehouse Directory and Configure Catalog\n",
        "\n",
        "We will create a temporary warehouse directory to store Iceberg data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created warehouse directory: /tmp/pyiceberg_warehouse\n",
            "Successfully configured catalog!\n"
          ]
        }
      ],
      "source": [
        "# Create warehouse directory\n",
        "warehouse_path = \"/tmp/pyiceberg_warehouse\"\n",
        "os.makedirs(warehouse_path, exist_ok=True)\n",
        "print(f\"Created warehouse directory: {warehouse_path}\")\n",
        "\n",
        "# Configure SQL catalog with SQLite\n",
        "catalog = load_catalog(\n",
        "    \"default\",\n",
        "    **{\n",
        "        'type': 'sql',\n",
        "        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n",
        "        \"warehouse\": f\"file://{warehouse_path}\",\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"Successfully configured catalog!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Namespace and Prepare Sample Data\n",
        "\n",
        "Before creating tables, we need to create a namespace and prepare sample data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created namespace 'default'\n",
            "Created sample data:\n",
            "pyarrow.Table\n",
            "VendorID: int64\n",
            "tpep_pickup_datetime: string\n",
            "tpep_dropoff_datetime: string\n",
            "passenger_count: double\n",
            "trip_distance: double\n",
            "RatecodeID: double\n",
            "store_and_fwd_flag: string\n",
            "PULocationID: int64\n",
            "DOLocationID: int64\n",
            "payment_type: int64\n",
            "fare_amount: double\n",
            "extra: double\n",
            "mta_tax: double\n",
            "tip_amount: double\n",
            "tolls_amount: double\n",
            "improvement_surcharge: double\n",
            "total_amount: double\n",
            "congestion_surcharge: double\n",
            "airport_fee: double\n",
            "----\n",
            "VendorID: [[1,1,2,2,1]]\n",
            "tpep_pickup_datetime: [[\"2023-01-01 00:00:00\",\"2023-01-01 00:15:00\",\"2023-01-01 00:30:00\",\"2023-01-01 00:45:00\",\"2023-01-01 01:00:00\"]]\n",
            "tpep_dropoff_datetime: [[\"2023-01-01 00:10:00\",\"2023-01-01 00:25:00\",\"2023-01-01 00:40:00\",\"2023-01-01 00:55:00\",\"2023-01-01 01:10:00\"]]\n",
            "passenger_count: [[1,2,1,3,2]]\n",
            "trip_distance: [[1.5,2.3,0.8,3.2,1.9]]\n",
            "RatecodeID: [[1,1,1,1,1]]\n",
            "store_and_fwd_flag: [[\"N\",\"N\",\"N\",\"N\",\"N\"]]\n",
            "PULocationID: [[1,2,3,4,5]]\n",
            "DOLocationID: [[2,3,4,5,6]]\n",
            "payment_type: [[1,1,2,1,2]]\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# Create namespace 'default'\n",
        "try:\n",
        "    catalog.create_namespace(\"default\")\n",
        "    print(\"Created namespace 'default'\")\n",
        "except Exception as e:\n",
        "    print(f\"Namespace 'default' already exists or error: {e}\")\n",
        "\n",
        "# Create sample taxi trip data\n",
        "sample_data = {\n",
        "    'VendorID': [1, 1, 2, 2, 1],\n",
        "    'tpep_pickup_datetime': ['2023-01-01 00:00:00', '2023-01-01 00:15:00', '2023-01-01 00:30:00', '2023-01-01 00:45:00', '2023-01-01 01:00:00'],\n",
        "    'tpep_dropoff_datetime': ['2023-01-01 00:10:00', '2023-01-01 00:25:00', '2023-01-01 00:40:00', '2023-01-01 00:55:00', '2023-01-01 01:10:00'],\n",
        "    'passenger_count': [1.0, 2.0, 1.0, 3.0, 2.0],\n",
        "    'trip_distance': [1.5, 2.3, 0.8, 3.2, 1.9],\n",
        "    'RatecodeID': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    'store_and_fwd_flag': ['N', 'N', 'N', 'N', 'N'],\n",
        "    'PULocationID': [1, 2, 3, 4, 5],\n",
        "    'DOLocationID': [2, 3, 4, 5, 6],\n",
        "    'payment_type': [1, 1, 2, 1, 2],\n",
        "    'fare_amount': [5.5, 8.2, 3.2, 12.5, 7.8],\n",
        "    'extra': [0.5, 0.5, 0.5, 0.5, 0.5],\n",
        "    'mta_tax': [0.5, 0.5, 0.5, 0.5, 0.5],\n",
        "    'tip_amount': [1.0, 1.5, 0.5, 2.5, 1.2],\n",
        "    'tolls_amount': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    'improvement_surcharge': [0.3, 0.3, 0.3, 0.3, 0.3],\n",
        "    'total_amount': [7.8, 11.0, 5.0, 16.3, 10.3],\n",
        "    'congestion_surcharge': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    'airport_fee': [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "}\n",
        "\n",
        "# Convert to PyArrow Table\n",
        "df = pa.table(sample_data)\n",
        "print(\"Created sample data:\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Iceberg Table\n",
        "\n",
        "Now we will create an Iceberg table from the sample data schema:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully created Iceberg table 'default.taxi_dataset'!\n",
            "\n",
            "Table schema:\n",
            "table {\n",
            "  1: VendorID: optional long\n",
            "  2: tpep_pickup_datetime: optional string\n",
            "  3: tpep_dropoff_datetime: optional string\n",
            "  4: passenger_count: optional double\n",
            "  5: trip_distance: optional double\n",
            "  6: RatecodeID: optional double\n",
            "  7: store_and_fwd_flag: optional string\n",
            "  8: PULocationID: optional long\n",
            "  9: DOLocationID: optional long\n",
            "  10: payment_type: optional long\n",
            "  11: fare_amount: optional double\n",
            "  12: extra: optional double\n",
            "  13: mta_tax: optional double\n",
            "  14: tip_amount: optional double\n",
            "  15: tolls_amount: optional double\n",
            "  16: improvement_surcharge: optional double\n",
            "  17: total_amount: optional double\n",
            "  18: congestion_surcharge: optional double\n",
            "  19: airport_fee: optional double\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Create Iceberg table from PyArrow DataFrame schema\n",
        "table = catalog.create_table(\n",
        "    \"default.taxi_dataset\",\n",
        "    schema=df.schema,\n",
        ")\n",
        "\n",
        "print(\"Successfully created Iceberg table 'default.taxi_dataset'!\")\n",
        "print(\"\\nTable schema:\")\n",
        "print(table.schema())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Add Data to Table\n",
        "\n",
        "Now we will add data to the newly created table:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 5 records to table\n",
            "\n",
            "Data in table:\n",
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         1  2023-01-01 00:00:00   2023-01-01 00:10:00              1.0   \n",
            "1         1  2023-01-01 00:15:00   2023-01-01 00:25:00              2.0   \n",
            "2         2  2023-01-01 00:30:00   2023-01-01 00:40:00              1.0   \n",
            "3         2  2023-01-01 00:45:00   2023-01-01 00:55:00              3.0   \n",
            "4         1  2023-01-01 01:00:00   2023-01-01 01:10:00              2.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0            1.5         1.0                  N             1             2   \n",
            "1            2.3         1.0                  N             2             3   \n",
            "2            0.8         1.0                  N             3             4   \n",
            "3            3.2         1.0                  N             4             5   \n",
            "4            1.9         1.0                  N             5             6   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             1          5.5    0.5      0.5         1.0           0.0   \n",
            "1             1          8.2    0.5      0.5         1.5           0.0   \n",
            "2             2          3.2    0.5      0.5         0.5           0.0   \n",
            "3             1         12.5    0.5      0.5         2.5           0.0   \n",
            "4             2          7.8    0.5      0.5         1.2           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
            "0                    0.3           7.8                   0.0          0.0  \n",
            "1                    0.3          11.0                   0.0          0.0  \n",
            "2                    0.3           5.0                   0.0          0.0  \n",
            "3                    0.3          16.3                   0.0          0.0  \n",
            "4                    0.3          10.3                   0.0          0.0  \n"
          ]
        }
      ],
      "source": [
        "# Add data to table\n",
        "table.append(df)\n",
        "\n",
        "# Check number of records added\n",
        "result_df = table.scan().to_arrow()\n",
        "print(f\"Added {len(result_df)} records to table\")\n",
        "print(\"\\nData in table:\")\n",
        "print(result_df.to_pandas())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Query and Analyze Data\n",
        "\n",
        "We will perform some basic queries on the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Query 1: All data ===\n",
            "Number of records: 5\n",
            "\n",
            "=== Query 2: Filter by fare_amount > 7 ===\n",
            "Number of records with fare_amount > 7: 3\n",
            "   VendorID  fare_amount  trip_distance  total_amount\n",
            "0         1          8.2            2.3          11.0\n",
            "1         2         12.5            3.2          16.3\n",
            "2         1          7.8            1.9          10.3\n",
            "\n",
            "=== Query 3: Filter by trip_distance >= 2 ===\n",
            "Number of records with trip_distance >= 2: 2\n",
            "   VendorID  trip_distance  fare_amount\n",
            "0         1            2.3          8.2\n",
            "1         2            3.2         12.5\n"
          ]
        }
      ],
      "source": [
        "# Query 1: Get all data\n",
        "print(\"=== Query 1: All data ===\")\n",
        "all_data = table.scan().to_arrow()\n",
        "print(f\"Number of records: {len(all_data)}\")\n",
        "\n",
        "# Query 2: Filter by fare_amount > 7\n",
        "print(\"\\n=== Query 2: Filter by fare_amount > 7 ===\")\n",
        "filtered_data = table.scan(row_filter=\"fare_amount > 7\").to_arrow()\n",
        "print(f\"Number of records with fare_amount > 7: {len(filtered_data)}\")\n",
        "if len(filtered_data) > 0:\n",
        "    print(filtered_data.to_pandas()[['VendorID', 'fare_amount', 'trip_distance', 'total_amount']])\n",
        "\n",
        "# Query 3: Filter by trip_distance\n",
        "print(\"\\n=== Query 3: Filter by trip_distance >= 2 ===\")\n",
        "distance_filtered = table.scan(row_filter=\"trip_distance >= 2\").to_arrow()\n",
        "print(f\"Number of records with trip_distance >= 2: {len(distance_filtered)}\")\n",
        "if len(distance_filtered) > 0:\n",
        "    print(distance_filtered.to_pandas()[['VendorID', 'trip_distance', 'fare_amount']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Schema Evolution - Add New Column\n",
        "\n",
        "One of the powerful features of Iceberg is schema evolution. We will add a new column to the table:\n",
        "\n",
        "**Important Note:** In Iceberg, you must update the schema before you can add data with new columns. Here's the correct way to do it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame with new column 'tip_per_mile':\n",
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         1  2023-01-01 00:00:00   2023-01-01 00:10:00              1.0   \n",
            "1         1  2023-01-01 00:15:00   2023-01-01 00:25:00              2.0   \n",
            "2         2  2023-01-01 00:30:00   2023-01-01 00:40:00              1.0   \n",
            "3         2  2023-01-01 00:45:00   2023-01-01 00:55:00              3.0   \n",
            "4         1  2023-01-01 01:00:00   2023-01-01 01:10:00              2.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0            1.5         1.0                  N             1             2   \n",
            "1            2.3         1.0                  N             2             3   \n",
            "2            0.8         1.0                  N             3             4   \n",
            "3            3.2         1.0                  N             4             5   \n",
            "4            1.9         1.0                  N             5             6   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             1          5.5    0.5      0.5         1.0           0.0   \n",
            "1             1          8.2    0.5      0.5         1.5           0.0   \n",
            "2             2          3.2    0.5      0.5         0.5           0.0   \n",
            "3             1         12.5    0.5      0.5         2.5           0.0   \n",
            "4             2          7.8    0.5      0.5         1.2           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \\\n",
            "0                    0.3           7.8                   0.0          0.0   \n",
            "1                    0.3          11.0                   0.0          0.0   \n",
            "2                    0.3           5.0                   0.0          0.0   \n",
            "3                    0.3          16.3                   0.0          0.0   \n",
            "4                    0.3          10.3                   0.0          0.0   \n",
            "\n",
            "   tip_per_mile  \n",
            "0      0.666667  \n",
            "1      0.652174  \n",
            "2      0.625000  \n",
            "3      0.781250  \n",
            "4      0.631579  \n",
            "\n",
            "New table schema after adding column:\n",
            "table {\n",
            "  1: VendorID: optional long\n",
            "  2: tpep_pickup_datetime: optional string\n",
            "  3: tpep_dropoff_datetime: optional string\n",
            "  4: passenger_count: optional double\n",
            "  5: trip_distance: optional double\n",
            "  6: RatecodeID: optional double\n",
            "  7: store_and_fwd_flag: optional string\n",
            "  8: PULocationID: optional long\n",
            "  9: DOLocationID: optional long\n",
            "  10: payment_type: optional long\n",
            "  11: fare_amount: optional double\n",
            "  12: extra: optional double\n",
            "  13: mta_tax: optional double\n",
            "  14: tip_amount: optional double\n",
            "  15: tolls_amount: optional double\n",
            "  16: improvement_surcharge: optional double\n",
            "  17: total_amount: optional double\n",
            "  18: congestion_surcharge: optional double\n",
            "  19: airport_fee: optional double\n",
            "  20: tip_per_mile: optional double\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Add new column 'tip_per_mile' to DataFrame\n",
        "df_with_new_column = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))\n",
        "\n",
        "print(\"DataFrame with new column 'tip_per_mile':\")\n",
        "print(df_with_new_column.to_pandas())\n",
        "\n",
        "# UPDATE SCHEMA BEFORE OVERWRITE\n",
        "# Add new column to table schema\n",
        "from pyiceberg.schema import Schema\n",
        "from pyiceberg.types import DoubleType, NestedField\n",
        "\n",
        "# Get current schema\n",
        "current_schema = table.schema()\n",
        "\n",
        "# Create new field for tip_per_mile column\n",
        "new_field = NestedField(\n",
        "    field_id=len(current_schema.fields) + 1,  # Next ID\n",
        "    name=\"tip_per_mile\",\n",
        "    field_type=DoubleType(),\n",
        "    required=False  # Can be null\n",
        ")\n",
        "\n",
        "# Create new schema with added column\n",
        "new_schema = Schema(*current_schema.fields, new_field)\n",
        "\n",
        "# Update table schema\n",
        "table.update_schema().add_column(\"tip_per_mile\", DoubleType()).commit()\n",
        "\n",
        "print(\"\\nNew table schema after adding column:\")\n",
        "print(table.schema())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Successfully overwrote with new schema!\n",
            "\n",
            "Data after overwrite:\n",
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         1  2023-01-01 00:00:00   2023-01-01 00:10:00              1.0   \n",
            "1         1  2023-01-01 00:15:00   2023-01-01 00:25:00              2.0   \n",
            "2         2  2023-01-01 00:30:00   2023-01-01 00:40:00              1.0   \n",
            "3         2  2023-01-01 00:45:00   2023-01-01 00:55:00              3.0   \n",
            "4         1  2023-01-01 01:00:00   2023-01-01 01:10:00              2.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0            1.5         1.0                  N             1             2   \n",
            "1            2.3         1.0                  N             2             3   \n",
            "2            0.8         1.0                  N             3             4   \n",
            "3            3.2         1.0                  N             4             5   \n",
            "4            1.9         1.0                  N             5             6   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             1          5.5    0.5      0.5         1.0           0.0   \n",
            "1             1          8.2    0.5      0.5         1.5           0.0   \n",
            "2             2          3.2    0.5      0.5         0.5           0.0   \n",
            "3             1         12.5    0.5      0.5         2.5           0.0   \n",
            "4             2          7.8    0.5      0.5         1.2           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \\\n",
            "0                    0.3           7.8                   0.0          0.0   \n",
            "1                    0.3          11.0                   0.0          0.0   \n",
            "2                    0.3           5.0                   0.0          0.0   \n",
            "3                    0.3          16.3                   0.0          0.0   \n",
            "4                    0.3          10.3                   0.0          0.0   \n",
            "\n",
            "   tip_per_mile  \n",
            "0      0.666667  \n",
            "1      0.652174  \n",
            "2      0.625000  \n",
            "3      0.781250  \n",
            "4      0.631579  \n"
          ]
        }
      ],
      "source": [
        "# Now we can overwrite with data containing the new column\n",
        "table.overwrite(df_with_new_column)\n",
        "\n",
        "print(\"✅ Successfully overwrote with new schema!\")\n",
        "print(\"\\nData after overwrite:\")\n",
        "result = table.scan().to_arrow()\n",
        "print(result.to_pandas())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Query with new column 'tip_per_mile' ===\n",
            "Data with new column:\n",
            "   VendorID  tip_amount  trip_distance  tip_per_mile\n",
            "0         1         1.0            1.5      0.666667\n",
            "1         1         1.5            2.3      0.652174\n",
            "2         2         0.5            0.8      0.625000\n",
            "3         2         2.5            3.2      0.781250\n",
            "4         1         1.2            1.9      0.631579\n",
            "\n",
            "=== Filter by tip_per_mile > 0.5 ===\n",
            "Number of records with tip_per_mile > 0.5: 5\n",
            "   VendorID  tip_per_mile  fare_amount\n",
            "0         1      0.666667          5.5\n",
            "1         1      0.652174          8.2\n",
            "2         2      0.625000          3.2\n",
            "3         2      0.781250         12.5\n",
            "4         1      0.631579          7.8\n"
          ]
        }
      ],
      "source": [
        "# Query with new column\n",
        "print(\"=== Query with new column 'tip_per_mile' ===\")\n",
        "evolved_data = table.scan().to_arrow()\n",
        "print(\"Data with new column:\")\n",
        "print(evolved_data.to_pandas()[['VendorID', 'tip_amount', 'trip_distance', 'tip_per_mile']])\n",
        "\n",
        "# Filter by tip_per_mile > 0.5\n",
        "print(\"\\n=== Filter by tip_per_mile > 0.5 ===\")\n",
        "tip_filtered = table.scan(row_filter=\"tip_per_mile > 0.5\").to_arrow()\n",
        "print(f\"Number of records with tip_per_mile > 0.5: {len(tip_filtered)}\")\n",
        "if len(tip_filtered) > 0:\n",
        "    print(tip_filtered.to_pandas()[['VendorID', 'tip_per_mile', 'fare_amount']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 Explanation of Schema Evolution\n",
        "\n",
        "**Why do we need to update schema first?**\n",
        "\n",
        "1. **Iceberg requires explicit schema evolution**: Cannot add data with new columns without updating schema first\n",
        "2. **Type safety**: Iceberg ensures type consistency across snapshots\n",
        "3. **Metadata management**: Schema changes are tracked in metadata files\n",
        "\n",
        "**Ways to perform schema evolution:**\n",
        "\n",
        "```python\n",
        "# Method 1: Using update_schema() (Recommended)\n",
        "table.update_schema().add_column(\"new_column\", DoubleType()).commit()\n",
        "\n",
        "# Method 2: Using union_by_name (for PyArrow)\n",
        "pa_table = pa.table.union_by_name([old_table, new_table])\n",
        "\n",
        "# Method 3: Create completely new schema\n",
        "new_schema = Schema(*old_fields, new_field)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Snapshot Management and History\n",
        "\n",
        "Iceberg supports time travel and snapshot management. Let's look at the table history:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Snapshot History ===\n",
            "          made_current_at          snapshot_id     parent_id  \\\n",
            "0 2025-09-17 04:10:01.752  3823500475119804241           NaN   \n",
            "1 2025-09-17 04:10:04.013  4201636063013981244  3.823500e+18   \n",
            "2 2025-09-17 04:10:04.026  5571586115741167391  4.201636e+18   \n",
            "\n",
            "   is_current_ancestor  \n",
            "0                 True  \n",
            "1                 True  \n",
            "2                 True  \n",
            "\n",
            "=== File Information ===\n",
            "Number of files: 1\n",
            "File information:\n",
            "                                           file_path  record_count  \\\n",
            "0  file:///tmp/pyiceberg_warehouse/default/taxi_d...             5   \n",
            "\n",
            "   file_size_in_bytes  \n",
            "0                8438  \n"
          ]
        }
      ],
      "source": [
        "# View snapshot history\n",
        "print(\"=== Snapshot History ===\")\n",
        "history = table.inspect.history()\n",
        "print(history.to_pandas())\n",
        "\n",
        "# View file information in table\n",
        "print(\"\\n=== File Information ===\")\n",
        "files_info = table.inspect.files()\n",
        "print(f\"Number of files: {len(files_info)}\")\n",
        "if len(files_info) > 0:\n",
        "    print(\"File information:\")\n",
        "    print(files_info.to_pandas()[['file_path', 'record_count', 'file_size_in_bytes']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Add New Data and Append Operations\n",
        "\n",
        "We will add more new data to the table to demonstrate append operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records after append: 8\n",
            "\n",
            "Final data:\n",
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         2  2023-01-01 01:15:00   2023-01-01 01:25:00              1.0   \n",
            "1         1  2023-01-01 01:30:00   2023-01-01 01:40:00              2.0   \n",
            "2         2  2023-01-01 01:45:00   2023-01-01 01:55:00              1.0   \n",
            "3         1  2023-01-01 00:00:00   2023-01-01 00:10:00              1.0   \n",
            "4         1  2023-01-01 00:15:00   2023-01-01 00:25:00              2.0   \n",
            "5         2  2023-01-01 00:30:00   2023-01-01 00:40:00              1.0   \n",
            "6         2  2023-01-01 00:45:00   2023-01-01 00:55:00              3.0   \n",
            "7         1  2023-01-01 01:00:00   2023-01-01 01:10:00              2.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0            2.1         1.0                  N             6             7   \n",
            "1            1.7         1.0                  N             7             8   \n",
            "2            2.8         1.0                  N             8             9   \n",
            "3            1.5         1.0                  N             1             2   \n",
            "4            2.3         1.0                  N             2             3   \n",
            "5            0.8         1.0                  N             3             4   \n",
            "6            3.2         1.0                  N             4             5   \n",
            "7            1.9         1.0                  N             5             6   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             1          9.2    0.5      0.5         1.8           0.0   \n",
            "1             2          6.8    0.5      0.5         1.2           0.0   \n",
            "2             1         11.5    0.5      0.5         2.3           0.0   \n",
            "3             1          5.5    0.5      0.5         1.0           0.0   \n",
            "4             1          8.2    0.5      0.5         1.5           0.0   \n",
            "5             2          3.2    0.5      0.5         0.5           0.0   \n",
            "6             1         12.5    0.5      0.5         2.5           0.0   \n",
            "7             2          7.8    0.5      0.5         1.2           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \\\n",
            "0                    0.3          12.3                   0.0          0.0   \n",
            "1                    0.3           9.3                   0.0          0.0   \n",
            "2                    0.3          15.1                   0.0          0.0   \n",
            "3                    0.3           7.8                   0.0          0.0   \n",
            "4                    0.3          11.0                   0.0          0.0   \n",
            "5                    0.3           5.0                   0.0          0.0   \n",
            "6                    0.3          16.3                   0.0          0.0   \n",
            "7                    0.3          10.3                   0.0          0.0   \n",
            "\n",
            "   tip_per_mile  \n",
            "0      0.860000  \n",
            "1      0.710000  \n",
            "2      0.820000  \n",
            "3      0.666667  \n",
            "4      0.652174  \n",
            "5      0.625000  \n",
            "6      0.781250  \n",
            "7      0.631579  \n"
          ]
        }
      ],
      "source": [
        "# Create new data to append\n",
        "new_sample_data = {\n",
        "    'VendorID': [2, 1, 2],\n",
        "    'tpep_pickup_datetime': ['2023-01-01 01:15:00', '2023-01-01 01:30:00', '2023-01-01 01:45:00'],\n",
        "    'tpep_dropoff_datetime': ['2023-01-01 01:25:00', '2023-01-01 01:40:00', '2023-01-01 01:55:00'],\n",
        "    'passenger_count': [1.0, 2.0, 1.0],\n",
        "    'trip_distance': [2.1, 1.7, 2.8],\n",
        "    'RatecodeID': [1.0, 1.0, 1.0],\n",
        "    'store_and_fwd_flag': ['N', 'N', 'N'],\n",
        "    'PULocationID': [6, 7, 8],\n",
        "    'DOLocationID': [7, 8, 9],\n",
        "    'payment_type': [1, 2, 1],\n",
        "    'fare_amount': [9.2, 6.8, 11.5],\n",
        "    'extra': [0.5, 0.5, 0.5],\n",
        "    'mta_tax': [0.5, 0.5, 0.5],\n",
        "    'tip_amount': [1.8, 1.2, 2.3],\n",
        "    'tolls_amount': [0.0, 0.0, 0.0],\n",
        "    'improvement_surcharge': [0.3, 0.3, 0.3],\n",
        "    'total_amount': [12.3, 9.3, 15.1],\n",
        "    'congestion_surcharge': [0.0, 0.0, 0.0],\n",
        "    'airport_fee': [0.0, 0.0, 0.0],\n",
        "    'tip_per_mile': [0.86, 0.71, 0.82]  # Calculate tip_per_mile for new data\n",
        "}\n",
        "\n",
        "# Convert to PyArrow Table\n",
        "new_df = pa.table(new_sample_data)\n",
        "\n",
        "# Append new data\n",
        "table.append(new_df)\n",
        "\n",
        "# Check total records after append\n",
        "final_data = table.scan().to_arrow()\n",
        "print(f\"Total records after append: {len(final_data)}\")\n",
        "print(\"\\nFinal data:\")\n",
        "print(final_data.to_pandas())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Statistical Analysis and Summary\n",
        "\n",
        "Finally, we will perform some basic statistical analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== OVERVIEW STATISTICS ===\n",
            "Total trips: 8\n",
            "Total revenue: $87.10\n",
            "Average revenue: $10.89\n",
            "Average tip: $1.50\n",
            "Average distance: 2.04 miles\n",
            "\n",
            "=== VENDOR ANALYSIS ===\n",
            "         total_amount              tip_amount trip_distance\n",
            "                count   sum   mean       mean          mean\n",
            "VendorID                                                   \n",
            "1                   4  38.4   9.60       1.23          1.85\n",
            "2                   4  48.7  12.18       1.78          2.22\n",
            "\n",
            "=== TOP 3 TRIPS WITH HIGHEST TIPS ===\n",
            "   VendorID  tip_amount  trip_distance  total_amount\n",
            "6         2         2.5            3.2          16.3\n",
            "2         2         2.3            2.8          15.1\n",
            "0         2         1.8            2.1          12.3\n",
            "\n",
            "=== TIP PER MILE ANALYSIS ===\n",
            "Average tip per mile: 0.72\n",
            "Highest tip per mile: 0.86\n",
            "Lowest tip per mile: 0.62\n"
          ]
        }
      ],
      "source": [
        "# Basic statistical analysis\n",
        "final_df = final_data.to_pandas()\n",
        "\n",
        "print(\"=== OVERVIEW STATISTICS ===\")\n",
        "print(f\"Total trips: {len(final_df)}\")\n",
        "print(f\"Total revenue: ${final_df['total_amount'].sum():.2f}\")\n",
        "print(f\"Average revenue: ${final_df['total_amount'].mean():.2f}\")\n",
        "print(f\"Average tip: ${final_df['tip_amount'].mean():.2f}\")\n",
        "print(f\"Average distance: {final_df['trip_distance'].mean():.2f} miles\")\n",
        "\n",
        "print(\"\\n=== VENDOR ANALYSIS ===\")\n",
        "vendor_stats = final_df.groupby('VendorID').agg({\n",
        "    'total_amount': ['count', 'sum', 'mean'],\n",
        "    'tip_amount': 'mean',\n",
        "    'trip_distance': 'mean'\n",
        "}).round(2)\n",
        "print(vendor_stats)\n",
        "\n",
        "print(\"\\n=== TOP 3 TRIPS WITH HIGHEST TIPS ===\")\n",
        "top_tips = final_df.nlargest(3, 'tip_amount')[['VendorID', 'tip_amount', 'trip_distance', 'total_amount']]\n",
        "print(top_tips)\n",
        "\n",
        "print(\"\\n=== TIP PER MILE ANALYSIS ===\")\n",
        "print(f\"Average tip per mile: {final_df['tip_per_mile'].mean():.2f}\")\n",
        "print(f\"Highest tip per mile: {final_df['tip_per_mile'].max():.2f}\")\n",
        "print(f\"Lowest tip per mile: {final_df['tip_per_mile'].min():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Check Warehouse Structure\n",
        "\n",
        "Finally, let's examine the warehouse structure to understand how Iceberg stores data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== WAREHOUSE STRUCTURE ===\n",
            "Warehouse path: /tmp/pyiceberg_warehouse\n",
            "📁 default/\n",
            "  📁 taxi_dataset/\n",
            "    📁 data/\n",
            "      📄 00000-0-b158b449-56df-46b2-9dc8-f8c9fcee9276.parquet (8016 bytes)\n",
            "      📄 00000-0-d4877cfd-8619-440e-9594-f4d8a4777f26.parquet (8438 bytes)\n",
            "      📄 00000-0-d5914a35-e2a6-4a5e-85a6-b069b1dd879d.parquet (8343 bytes)\n",
            "    📁 metadata/\n",
            "      📄 00000-b7aea812-9611-4e0b-b606-c1ffca6a1b9b.metadata.json (1836 bytes)\n",
            "      📄 00001-b795de7b-9b6f-4505-991e-7965afd53ab4.metadata.json (2668 bytes)\n",
            "      📄 00002-fe12bb19-a23e-4499-9741-0e18fdd7f677.metadata.json (4210 bytes)\n",
            "      📄 00003-1d8e8f2c-2da2-41b2-8e22-f88b84ceb708.metadata.json (5597 bytes)\n",
            "      📄 00004-f631d466-6264-4ade-9b47-9029a1decbbb.metadata.json (6374 bytes)\n",
            "      📄 091cfe63-6336-4a48-817a-3aa3a4e5503b-m0.avro (5737 bytes)\n",
            "      📄 b158b449-56df-46b2-9dc8-f8c9fcee9276-m0.avro (5672 bytes)\n",
            "      📄 d4877cfd-8619-440e-9594-f4d8a4777f26-m0.avro (5754 bytes)\n",
            "      📄 d5914a35-e2a6-4a5e-85a6-b069b1dd879d-m0.avro (5759 bytes)\n",
            "      📄 snap-3823500475119804241-0-b158b449-56df-46b2-9dc8-f8c9fcee9276.avro (1794 bytes)\n",
            "      📄 snap-4201636063013981244-0-091cfe63-6336-4a48-817a-3aa3a4e5503b.avro (1809 bytes)\n",
            "      📄 snap-4877304003057618472-0-d5914a35-e2a6-4a5e-85a6-b069b1dd879d.avro (1866 bytes)\n",
            "      📄 snap-5571586115741167391-0-d4877cfd-8619-440e-9594-f4d8a4777f26.avro (1810 bytes)\n",
            "📄 pyiceberg_catalog.db (20480 bytes)\n"
          ]
        }
      ],
      "source": [
        "# Check warehouse structure\n",
        "import os\n",
        "\n",
        "print(\"=== WAREHOUSE STRUCTURE ===\")\n",
        "print(f\"Warehouse path: {warehouse_path}\")\n",
        "\n",
        "def explore_directory(path, prefix=\"\"):\n",
        "    \"\"\"Recursively explore directory structure\"\"\"\n",
        "    try:\n",
        "        items = os.listdir(path)\n",
        "        for item in sorted(items):\n",
        "            item_path = os.path.join(path, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                print(f\"{prefix}📁 {item}/\")\n",
        "                explore_directory(item_path, prefix + \"  \")\n",
        "            else:\n",
        "                size = os.path.getsize(item_path)\n",
        "                print(f\"{prefix}📄 {item} ({size} bytes)\")\n",
        "    except PermissionError:\n",
        "        print(f\"{prefix}❌ Permission denied\")\n",
        "\n",
        "explore_directory(warehouse_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this lab, we have learned:\n",
        "\n",
        "### ✅ What we accomplished:\n",
        "1. **Installed PyIceberg** with necessary dependencies\n",
        "2. **Configured SQL Catalog** with SQLite for development\n",
        "3. **Created Iceberg table** from PyArrow schema\n",
        "4. **Added and managed data** with append operations\n",
        "5. **Performed queries** with filtering\n",
        "6. **Schema evolution** - added new column to table\n",
        "7. **Snapshot management** and history tracking\n",
        "8. **Statistical analysis** of taxi trip data\n",
        "\n",
        "### 🎯 Key features of Apache Iceberg:\n",
        "- **Schema Evolution**: Add/remove/modify columns without rewriting entire data\n",
        "- **Time Travel**: Access data at different points in time\n",
        "- **ACID Transactions**: Ensure data consistency\n",
        "- **Partitioning**: Optimize query performance\n",
        "- **Metadata Management**: Efficient metadata management\n",
        "\n",
        "### 🚀 Next steps:\n",
        "- Experiment with larger datasets\n",
        "- Configure with cloud storage (S3, GCS, Azure)\n",
        "- Integrate with other query engines (Spark, Trino, etc.)\n",
        "- Implement partitioning strategies\n",
        "\n",
        "**Congratulations! You have completed the basic PyIceberg lab! 🎉**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
