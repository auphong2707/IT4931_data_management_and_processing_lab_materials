{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Time Travel & Versioning Lab - Apache Iceberg\n",
        "\n",
        "## 🎯 Lab Objectives\n",
        "\n",
        "In this lab, we will explore Apache Iceberg's powerful time travel and versioning capabilities:\n",
        "\n",
        "1. **Snapshot Management**: Create and manage data snapshots\n",
        "2. **Time Travel Queries**: Query data at specific points in time\n",
        "3. **Rollback Operations**: Rollback to previous versions\n",
        "4. **Version Comparison**: Compare data between different versions\n",
        "5. **Historical Analysis**: Analyze data changes over time\n",
        "6. **Production Scenarios**: Apply time travel to real-world use cases\n",
        "\n",
        "## 🏗️ Time Travel Architecture\n",
        "\n",
        "### Iceberg Time Travel Features:\n",
        "- **Snapshot-based Versioning**: Each write creates a new snapshot\n",
        "- **Time Travel Queries**: Query data as it existed at any point in time\n",
        "- **Rollback Capabilities**: Restore data to previous states\n",
        "- **Version Comparison**: Compare data between snapshots\n",
        "- **Historical Metadata**: Track all changes and operations\n",
        "\n",
        "### Use Cases:\n",
        "- **Data Recovery**: Restore accidentally deleted or corrupted data\n",
        "- **Audit Trails**: Track all changes for compliance and auditing\n",
        "- **A/B Testing**: Compare different versions of datasets\n",
        "- **Debugging**: Investigate data issues by examining historical states\n",
        "- **Compliance**: Meet regulatory requirements for data retention\n",
        "\n",
        "## 📊 Dataset: E-commerce Customer Data\n",
        "\n",
        "We will work with evolving customer data including:\n",
        "- **Customer Profiles**: Personal and demographic information\n",
        "- **Purchase History**: Transaction records over time\n",
        "- **Account Status**: Active/inactive status changes\n",
        "- **Data Evolution**: Schema and data changes over time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Successfully imported all libraries!\n",
            "📦 PyArrow version: 21.0.0\n",
            "📦 Pandas version: 2.3.2\n",
            "📦 NumPy version: 2.2.6\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# PyIceberg imports\n",
        "from pyiceberg.catalog import load_catalog\n",
        "from pyiceberg.schema import Schema\n",
        "from pyiceberg.types import (\n",
        "    StructType, StringType, IntegerType, LongType, DoubleType, BooleanType,\n",
        "    TimestampType, DateType, NestedField\n",
        ")\n",
        "\n",
        "# Data processing\n",
        "import pyarrow as pa\n",
        "import pyarrow.compute as pc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"✅ Successfully imported all libraries!\")\n",
        "print(f\"📦 PyArrow version: {pa.__version__}\")\n",
        "print(f\"📦 Pandas version: {pd.__version__}\")\n",
        "print(f\"📦 NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Warehouse and Catalog Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ℹ️  Namespace 'ecommerce' already exists: Namespace ecommerce already exists\n",
            "📁 Warehouse path: /tmp/timetravel_iceberg_warehouse\n",
            "🎯 Ready for Time Travel Lab!\n"
          ]
        }
      ],
      "source": [
        "# Setup warehouse and catalog\n",
        "warehouse_path = \"/tmp/timetravel_iceberg_warehouse\"\n",
        "os.makedirs(warehouse_path, exist_ok=True)\n",
        "\n",
        "# Configure catalog\n",
        "catalog = load_catalog(\n",
        "    \"timetravel\",\n",
        "    **{\n",
        "        'type': 'sql',\n",
        "        \"uri\": f\"sqlite:///{warehouse_path}/timetravel_catalog.db\",\n",
        "        \"warehouse\": f\"file://{warehouse_path}\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Create namespace\n",
        "try:\n",
        "    catalog.create_namespace(\"ecommerce\")\n",
        "    print(\"✅ Created namespace 'ecommerce'\")\n",
        "except Exception as e:\n",
        "    print(f\"ℹ️  Namespace 'ecommerce' already exists: {e}\")\n",
        "\n",
        "print(f\"📁 Warehouse path: {warehouse_path}\")\n",
        "print(\"🎯 Ready for Time Travel Lab!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate Evolving Customer Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Generating initial customer data...\n",
            "✅ Generated 1,000 customers\n",
            "\n",
            "📋 Sample customer data:\n",
            "  customer_id: CUST_00001\n",
            "  first_name: Customer1\n",
            "  last_name: LastName1\n",
            "  email: customer1@example.com\n",
            "  phone: +1-555-255-7057\n",
            "  country: UK\n",
            "  city: New York\n",
            "  age: 23\n",
            "  customer_segment: Budget\n",
            "  account_status: Pending\n",
            "  registration_date: 2023-02-01 00:00:00\n",
            "  last_login: 2023-06-24 00:00:00\n",
            "  total_purchases: 31\n",
            "  total_spent: 7774.29\n",
            "  loyalty_points: 2478\n",
            "  is_premium: True\n",
            "  newsletter_subscribed: True\n",
            "  created_at: 2025-02-06 13:52:03.064959\n",
            "  updated_at: 2025-09-17 13:52:03.064964\n",
            "\n",
            "📊 Dataset overview:\n",
            "  Total customers: 1,000\n",
            "  Countries: 8\n",
            "  Segments: 4\n",
            "  Statuses: 4\n",
            "  Date range: 2023-01-01 00:00:00 to 2023-06-30 00:00:00\n"
          ]
        }
      ],
      "source": [
        "def generate_customer_data(n_customers=1000):\n",
        "    \"\"\"Generate realistic customer data that will evolve over time\"\"\"\n",
        "    \n",
        "    customers = []\n",
        "    countries = [\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"Japan\", \"Australia\", \"Brazil\"]\n",
        "    cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\"]\n",
        "    segments = [\"Premium\", \"Standard\", \"Budget\", \"VIP\"]\n",
        "    statuses = [\"Active\", \"Inactive\", \"Suspended\", \"Pending\"]\n",
        "    \n",
        "    for i in range(n_customers):\n",
        "        # Generate customer with realistic data\n",
        "        customer = {\n",
        "            \"customer_id\": f\"CUST_{i+1:05d}\",\n",
        "            \"first_name\": f\"Customer{i+1}\",\n",
        "            \"last_name\": f\"LastName{i+1}\",\n",
        "            \"email\": f\"customer{i+1}@example.com\",\n",
        "            \"phone\": f\"+1-555-{random.randint(100, 999)}-{random.randint(1000, 9999)}\",\n",
        "            \"country\": random.choice(countries),\n",
        "            \"city\": random.choice(cities),\n",
        "            \"age\": random.randint(18, 80),\n",
        "            \"customer_segment\": random.choice(segments),\n",
        "            \"account_status\": random.choice(statuses),\n",
        "            \"registration_date\": datetime(2023, 1, 1) + timedelta(days=random.randint(0, 180)),\n",
        "            \"last_login\": datetime(2023, 6, 1) + timedelta(days=random.randint(0, 30)),\n",
        "            \"total_purchases\": random.randint(0, 50),\n",
        "            \"total_spent\": round(random.uniform(0, 10000), 2),\n",
        "            \"loyalty_points\": random.randint(0, 5000),\n",
        "            \"is_premium\": random.choice([True, False]),\n",
        "            \"newsletter_subscribed\": random.choice([True, False]),\n",
        "            \"created_at\": datetime.now() - timedelta(days=random.randint(1, 365)),\n",
        "            \"updated_at\": datetime.now()\n",
        "        }\n",
        "        \n",
        "        customers.append(customer)\n",
        "    \n",
        "    return customers\n",
        "\n",
        "# Generate initial customer data\n",
        "print(\"🔄 Generating initial customer data...\")\n",
        "customer_data = generate_customer_data(1000)\n",
        "print(f\"✅ Generated {len(customer_data):,} customers\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n📋 Sample customer data:\")\n",
        "sample_customer = customer_data[0]\n",
        "for key, value in sample_customer.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Convert to DataFrame for easier manipulation\n",
        "df_customers = pd.DataFrame(customer_data)\n",
        "print(f\"\\n📊 Dataset overview:\")\n",
        "print(f\"  Total customers: {len(df_customers):,}\")\n",
        "print(f\"  Countries: {df_customers['country'].nunique()}\")\n",
        "print(f\"  Segments: {df_customers['customer_segment'].nunique()}\")\n",
        "print(f\"  Statuses: {df_customers['account_status'].nunique()}\")\n",
        "print(f\"  Date range: {df_customers['registration_date'].min()} to {df_customers['registration_date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Customer Schema and Initial Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 FIXED Rollback Operations with Smart Schema Management\n",
            "\n",
            "🔄 Rollback Operation 1: Rolling back to Snapshot 3 (before deletions)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'customers_table' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔄 Rollback Operation 1: Rolling back to Snapshot 3 (before deletions)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Get current state before rollback\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m current_before_rollback \u001b[38;5;241m=\u001b[39m \u001b[43mcustomers_table\u001b[49m\u001b[38;5;241m.\u001b[39mscan()\u001b[38;5;241m.\u001b[39mto_arrow()\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Current records before rollback: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(current_before_rollback)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Get data from Snapshot 3 and overwrite current table\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'customers_table' is not defined"
          ]
        }
      ],
      "source": [
        "# FIXED Rollback Operations - Smart Schema Handling\n",
        "print(\"🔧 FIXED Rollback Operations with Smart Schema Management\")\n",
        "\n",
        "# Helper function to create PyArrow table with dynamic schema\n",
        "def create_pyarrow_table_with_dynamic_schema(df, include_credit_score=False):\n",
        "    \"\"\"Create PyArrow table with dynamic schema based on data availability\"\"\"\n",
        "    \n",
        "    # Base schema without credit_score\n",
        "    base_schema = pa.schema([\n",
        "        pa.field('customer_id', pa.string(), nullable=False),\n",
        "        pa.field('first_name', pa.string(), nullable=False),\n",
        "        pa.field('last_name', pa.string(), nullable=False),\n",
        "        pa.field('email', pa.string(), nullable=False),\n",
        "        pa.field('phone', pa.string(), nullable=False),\n",
        "        pa.field('country', pa.string(), nullable=False),\n",
        "        pa.field('city', pa.string(), nullable=False),\n",
        "        pa.field('age', pa.int32(), nullable=False),\n",
        "        pa.field('customer_segment', pa.string(), nullable=False),\n",
        "        pa.field('account_status', pa.string(), nullable=False),\n",
        "        pa.field('registration_date', pa.timestamp('us'), nullable=False),\n",
        "        pa.field('last_login', pa.timestamp('us'), nullable=False),\n",
        "        pa.field('total_purchases', pa.int32(), nullable=False),\n",
        "        pa.field('total_spent', pa.float64(), nullable=False),\n",
        "        pa.field('loyalty_points', pa.int32(), nullable=False),\n",
        "        pa.field('is_premium', pa.bool_(), nullable=False),\n",
        "        pa.field('newsletter_subscribed', pa.bool_(), nullable=False),\n",
        "        pa.field('created_at', pa.timestamp('us'), nullable=False),\n",
        "        pa.field('updated_at', pa.timestamp('us'), nullable=False)\n",
        "    ])\n",
        "    \n",
        "    # Add credit_score if needed\n",
        "    if include_credit_score:\n",
        "        base_schema = base_schema.append(pa.field('credit_score', pa.int32(), nullable=False))\n",
        "    \n",
        "    # Convert timestamps\n",
        "    df_clean = df.copy()\n",
        "    timestamp_cols = ['registration_date', 'last_login', 'created_at', 'updated_at']\n",
        "    for col in timestamp_cols:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = pd.to_datetime(df_clean[col]).dt.floor('us')\n",
        "    \n",
        "    # Create table data dictionary\n",
        "    table_data = {}\n",
        "    for field in base_schema:\n",
        "        field_name = field.name\n",
        "        if field_name in df_clean.columns:\n",
        "            table_data[field_name] = df_clean[field_name]\n",
        "        elif field_name == 'credit_score' and include_credit_score:\n",
        "            # Generate random credit scores if not present\n",
        "            table_data[field_name] = np.random.randint(300, 850, len(df_clean))\n",
        "    \n",
        "    return pa.table(table_data, schema=base_schema)\n",
        "\n",
        "# Rollback Operation 1: Rollback to Snapshot 3 (before deletions)\n",
        "print(\"\\n🔄 Rollback Operation 1: Rolling back to Snapshot 3 (before deletions)\")\n",
        "\n",
        "# Get current state before rollback\n",
        "current_before_rollback = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Current records before rollback: {len(current_before_rollback):,}\")\n",
        "\n",
        "# Get data from Snapshot 3 and overwrite current table\n",
        "snapshot_3_id = snapshots_df.iloc[2]['snapshot_id']\n",
        "snapshot_3_data = customers_table.scan(snapshot_id=snapshot_3_id).to_arrow().to_pandas()\n",
        "\n",
        "# Convert back to PyArrow table for overwrite\n",
        "snapshot_3_table = create_pyarrow_table_with_dynamic_schema(snapshot_3_data, include_credit_score='credit_score' in snapshot_3_data.columns)\n",
        "customers_table.overwrite(snapshot_3_table)\n",
        "\n",
        "print(\"✅ Rollback completed!\")\n",
        "\n",
        "# Verify rollback\n",
        "current_after_rollback = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Records after rollback: {len(current_after_rollback):,}\")\n",
        "print(f\"  Records restored: {len(current_after_rollback) - len(current_before_rollback):,}\")\n",
        "\n",
        "# Check if deleted customers are back\n",
        "inactive_customers_restored = len(current_after_rollback[current_after_rollback['account_status'] == 'Inactive'])\n",
        "print(f\"  Inactive customers restored: {inactive_customers_restored:,}\")\n",
        "\n",
        "# Rollback Operation 2: Rollback to Snapshot 1 (original state)\n",
        "print(f\"\\n🔄 Rollback Operation 2: Rolling back to Snapshot 1 (original state)\")\n",
        "\n",
        "# Get current state\n",
        "current_before_rollback2 = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Current records before rollback: {len(current_before_rollback2):,}\")\n",
        "\n",
        "# IMPORTANT: First remove credit_score column from schema\n",
        "print(\"  Step 1: Removing credit_score column from schema...\")\n",
        "try:\n",
        "    customers_table.update_schema().remove_column(\"credit_score\").commit()\n",
        "    print(\"  ✅ Schema updated: credit_score column removed\")\n",
        "except Exception as e:\n",
        "    print(f\"  ℹ️ Schema update: {e}\")\n",
        "\n",
        "# Get data from Snapshot 1 and overwrite current table\n",
        "snapshot_1_id = snapshots_df.iloc[0]['snapshot_id']\n",
        "snapshot_1_data = customers_table.scan(snapshot_id=snapshot_1_id).to_arrow().to_pandas()\n",
        "\n",
        "# Convert back to PyArrow table for overwrite (without credit_score)\n",
        "snapshot_1_table = create_pyarrow_table_with_dynamic_schema(snapshot_1_data, include_credit_score=False)\n",
        "customers_table.overwrite(snapshot_1_table)\n",
        "\n",
        "print(\"✅ Rollback to original state completed!\")\n",
        "\n",
        "# Verify rollback\n",
        "current_after_rollback2 = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Records after rollback: {len(current_after_rollback2):,}\")\n",
        "print(f\"  Records restored: {len(current_after_rollback2) - len(current_before_rollback2):,}\")\n",
        "\n",
        "# Check if schema is back to original\n",
        "print(f\"  Columns after rollback: {list(current_after_rollback2.columns)}\")\n",
        "print(f\"  Has credit_score column: {'credit_score' in current_after_rollback2.columns}\")\n",
        "\n",
        "# Check premium customers (should be back to original count)\n",
        "premium_customers_restored = len(current_after_rollback2[current_after_rollback2['customer_segment'] == 'Premium'])\n",
        "print(f\"  Premium customers restored: {premium_customers_restored:,}\")\n",
        "\n",
        "# Rollback Operation 3: Rollback to Snapshot 5 (latest with all features)\n",
        "print(f\"\\n🔄 Rollback Operation 3: Rolling back to Snapshot 5 (latest with all features)\")\n",
        "\n",
        "# IMPORTANT: First restore the schema to include credit_score column\n",
        "print(\"  Step 1: Restoring schema with credit_score column...\")\n",
        "customers_table.update_schema().add_column(\"credit_score\", IntegerType()).commit()\n",
        "print(\"  ✅ Schema updated: credit_score column added\")\n",
        "\n",
        "# Get data from Snapshot 5 and overwrite current table\n",
        "snapshot_5_id = snapshots_df.iloc[4]['snapshot_id']\n",
        "snapshot_5_data = customers_table.scan(snapshot_id=snapshot_5_id).to_arrow().to_pandas()\n",
        "\n",
        "# Convert back to PyArrow table for overwrite (with credit_score column)\n",
        "snapshot_5_table = create_pyarrow_table_with_dynamic_schema(snapshot_5_data, include_credit_score=True)\n",
        "customers_table.overwrite(snapshot_5_table)\n",
        "\n",
        "print(\"✅ Rollback to latest state completed!\")\n",
        "\n",
        "# Verify rollback\n",
        "final_state = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Final records: {len(final_state):,}\")\n",
        "print(f\"  Final columns: {list(final_state.columns)}\")\n",
        "print(f\"  Has credit_score column: {'credit_score' in final_state.columns}\")\n",
        "\n",
        "# Display rollback summary\n",
        "print(f\"\\n📊 Rollback Operations Summary:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Rollback 1 (to Snapshot 3):\")\n",
        "print(f\"  Restored {len(current_after_rollback) - len(current_before_rollback):,} deleted customers\")\n",
        "print(f\"  Restored {inactive_customers_restored:,} inactive customers\")\n",
        "\n",
        "print(f\"\\nRollback 2 (to Snapshot 1):\")\n",
        "print(f\"  Restored {len(current_after_rollback2) - len(current_before_rollback2):,} customers\")\n",
        "print(f\"  Removed credit_score column from schema\")\n",
        "print(f\"  Reset premium customers to {premium_customers_restored:,}\")\n",
        "\n",
        "print(f\"\\nRollback 3 (to Snapshot 5):\")\n",
        "print(f\"  Restored schema with credit_score column\")\n",
        "print(f\"  Restored to latest state with all features\")\n",
        "print(f\"  Final records: {len(final_state):,}\")\n",
        "print(f\"  Final columns: {len(final_state.columns)}\")\n",
        "\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️ Creating customer schema...\n",
            "✅ Schema created successfully!\n",
            "\n",
            "🔄 Creating initial customer table (Snapshot 1)...\n",
            "🗑️ Dropped existing table\n",
            "✅ Initial table created with Snapshot 1!\n",
            "\n",
            "📸 Snapshot Information:\n",
            "  Total snapshots: 1\n",
            "  Available columns: ['committed_at', 'snapshot_id', 'parent_id', 'operation', 'manifest_list', 'summary']\n",
            "  Latest snapshot ID: 1783715834426628930\n",
            "  Latest snapshot timestamp: 2025-09-17 06:44:58.504000\n",
            "  Latest snapshot summary: [('added-files-size', '43582'), ('added-data-files', '1'), ('added-records', '1000'), ('total-data-files', '1'), ('total-delete-files', '0'), ('total-records', '1000'), ('total-files-size', '43582'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "  Current records: 1,000\n"
          ]
        }
      ],
      "source": [
        "# Create Iceberg schema for customer data\n",
        "def create_customer_schema():\n",
        "    \"\"\"Create Iceberg schema for customer data\"\"\"\n",
        "    \n",
        "    schema = Schema(\n",
        "        NestedField(1, \"customer_id\", StringType(), required=True),\n",
        "        NestedField(2, \"first_name\", StringType(), required=True),\n",
        "        NestedField(3, \"last_name\", StringType(), required=True),\n",
        "        NestedField(4, \"email\", StringType(), required=True),\n",
        "        NestedField(5, \"phone\", StringType(), required=True),\n",
        "        NestedField(6, \"country\", StringType(), required=True),\n",
        "        NestedField(7, \"city\", StringType(), required=True),\n",
        "        NestedField(8, \"age\", IntegerType(), required=True),\n",
        "        NestedField(9, \"customer_segment\", StringType(), required=True),\n",
        "        NestedField(10, \"account_status\", StringType(), required=True),\n",
        "        NestedField(11, \"registration_date\", TimestampType(), required=True),\n",
        "        NestedField(12, \"last_login\", TimestampType(), required=True),\n",
        "        NestedField(13, \"total_purchases\", IntegerType(), required=True),\n",
        "        NestedField(14, \"total_spent\", DoubleType(), required=True),\n",
        "        NestedField(15, \"loyalty_points\", IntegerType(), required=True),\n",
        "        NestedField(16, \"is_premium\", BooleanType(), required=True),\n",
        "        NestedField(17, \"newsletter_subscribed\", BooleanType(), required=True),\n",
        "        NestedField(18, \"created_at\", TimestampType(), required=True),\n",
        "        NestedField(19, \"updated_at\", TimestampType(), required=True)\n",
        "    )\n",
        "    \n",
        "    return schema\n",
        "\n",
        "# Create the schema\n",
        "print(\"🏗️ Creating customer schema...\")\n",
        "customer_schema = create_customer_schema()\n",
        "print(\"✅ Schema created successfully!\")\n",
        "\n",
        "# Helper function to create PyArrow table\n",
        "def create_customer_pyarrow_table(df):\n",
        "    \"\"\"Create PyArrow table with correct timestamp precision\"\"\"\n",
        "    \n",
        "    # Create PyArrow schema\n",
        "    pyarrow_schema = pa.schema([\n",
        "        pa.field('customer_id', pa.string(), nullable=False),\n",
        "        pa.field('first_name', pa.string(), nullable=False),\n",
        "        pa.field('last_name', pa.string(), nullable=False),\n",
        "        pa.field('email', pa.string(), nullable=False),\n",
        "        pa.field('phone', pa.string(), nullable=False),\n",
        "        pa.field('country', pa.string(), nullable=False),\n",
        "        pa.field('city', pa.string(), nullable=False),\n",
        "        pa.field('age', pa.int32(), nullable=False),\n",
        "        pa.field('customer_segment', pa.string(), nullable=False),\n",
        "        pa.field('account_status', pa.string(), nullable=False),\n",
        "        pa.field('registration_date', pa.timestamp('us'), nullable=False),\n",
        "        pa.field('last_login', pa.timestamp('us'), nullable=False),\n",
        "        pa.field('total_purchases', pa.int32(), nullable=False),\n",
        "        pa.field('total_spent', pa.float64(), nullable=False),\n",
        "        pa.field('loyalty_points', pa.int32(), nullable=False),\n",
        "        pa.field('is_premium', pa.bool_(), nullable=False),\n",
        "        pa.field('newsletter_subscribed', pa.bool_(), nullable=False),\n",
        "        pa.field('created_at', pa.timestamp('us'), nullable=False),\n",
        "        pa.field('updated_at', pa.timestamp('us'), nullable=False)\n",
        "    ])\n",
        "    \n",
        "    # Convert timestamps to microseconds\n",
        "    df_clean = df.copy()\n",
        "    timestamp_cols = ['registration_date', 'last_login', 'created_at', 'updated_at']\n",
        "    for col in timestamp_cols:\n",
        "        df_clean[col] = pd.to_datetime(df_clean[col]).dt.floor('us')\n",
        "    \n",
        "    # Create table with explicit schema\n",
        "    return pa.table({\n",
        "        'customer_id': df_clean['customer_id'],\n",
        "        'first_name': df_clean['first_name'],\n",
        "        'last_name': df_clean['last_name'],\n",
        "        'email': df_clean['email'],\n",
        "        'phone': df_clean['phone'],\n",
        "        'country': df_clean['country'],\n",
        "        'city': df_clean['city'],\n",
        "        'age': df_clean['age'],\n",
        "        'customer_segment': df_clean['customer_segment'],\n",
        "        'account_status': df_clean['account_status'],\n",
        "        'registration_date': df_clean['registration_date'],\n",
        "        'last_login': df_clean['last_login'],\n",
        "        'total_purchases': df_clean['total_purchases'],\n",
        "        'total_spent': df_clean['total_spent'],\n",
        "        'loyalty_points': df_clean['loyalty_points'],\n",
        "        'is_premium': df_clean['is_premium'],\n",
        "        'newsletter_subscribed': df_clean['newsletter_subscribed'],\n",
        "        'created_at': df_clean['created_at'],\n",
        "        'updated_at': df_clean['updated_at']\n",
        "    }, schema=pyarrow_schema)\n",
        "\n",
        "# Helper function to safely get snapshot info\n",
        "def get_snapshot_info(snapshots_df, snapshot_row):\n",
        "    \"\"\"Safely extract snapshot information from DataFrame row\"\"\"\n",
        "    info = {\n",
        "        'snapshot_id': snapshot_row['snapshot_id'],\n",
        "        'timestamp': None,\n",
        "        'summary': None\n",
        "    }\n",
        "    \n",
        "    # Try different timestamp column names\n",
        "    timestamp_cols = ['timestamp_ms', 'committed_at', 'timestamp', 'created_at']\n",
        "    for col in timestamp_cols:\n",
        "        if col in snapshots_df.columns:\n",
        "            info['timestamp'] = snapshot_row[col]\n",
        "            break\n",
        "    \n",
        "    # Try different summary column names\n",
        "    summary_cols = ['summary', 'description', 'operation']\n",
        "    for col in summary_cols:\n",
        "        if col in snapshots_df.columns:\n",
        "            info['summary'] = snapshot_row[col]\n",
        "            break\n",
        "    \n",
        "    return info\n",
        "\n",
        "# Create initial table (Snapshot 1)\n",
        "print(\"\\n🔄 Creating initial customer table (Snapshot 1)...\")\n",
        "\n",
        "# Drop existing table if it exists\n",
        "try:\n",
        "    catalog.drop_table(\"ecommerce.customers\")\n",
        "    print(\"🗑️ Dropped existing table\")\n",
        "except Exception as e:\n",
        "    print(f\"ℹ️ No existing table to drop: {e}\")\n",
        "\n",
        "# Create table\n",
        "customers_table = catalog.create_table(\n",
        "    \"ecommerce.customers\",\n",
        "    schema=customer_schema\n",
        ")\n",
        "\n",
        "# Convert data to PyArrow table\n",
        "customer_table = create_customer_pyarrow_table(df_customers)\n",
        "\n",
        "# Insert initial data\n",
        "customers_table.append(customer_table)\n",
        "\n",
        "print(\"✅ Initial table created with Snapshot 1!\")\n",
        "\n",
        "# Get snapshot information - Safe access\n",
        "snapshots_df = customers_table.inspect.snapshots().to_pandas()\n",
        "print(f\"\\n📸 Snapshot Information:\")\n",
        "print(f\"  Total snapshots: {len(snapshots_df)}\")\n",
        "print(f\"  Available columns: {list(snapshots_df.columns)}\")\n",
        "\n",
        "if len(snapshots_df) > 0:\n",
        "    latest_snapshot_info = get_snapshot_info(snapshots_df, snapshots_df.iloc[-1])\n",
        "    print(f\"  Latest snapshot ID: {latest_snapshot_info['snapshot_id']}\")\n",
        "    \n",
        "    if latest_snapshot_info['timestamp'] is not None:\n",
        "        if isinstance(latest_snapshot_info['timestamp'], (int, float)):\n",
        "            timestamp_str = datetime.fromtimestamp(latest_snapshot_info['timestamp'] / 1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        else:\n",
        "            timestamp_str = str(latest_snapshot_info['timestamp'])\n",
        "        print(f\"  Latest snapshot timestamp: {timestamp_str}\")\n",
        "    else:\n",
        "        print(f\"  Latest snapshot timestamp: Not available\")\n",
        "    \n",
        "    if latest_snapshot_info['summary'] is not None:\n",
        "        print(f\"  Latest snapshot summary: {latest_snapshot_info['summary']}\")\n",
        "    else:\n",
        "        print(f\"  Latest snapshot summary: Not available\")\n",
        "\n",
        "# Get current record count\n",
        "current_count = len(customers_table.scan().to_arrow())\n",
        "print(f\"  Current records: {current_count:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Snapshot Management - Creating Multiple Versions\n",
        "\n",
        "Now let's create multiple snapshots by making changes to our data over time. Each write operation creates a new snapshot.\n",
        "\n",
        "### 📸 **Snapshot Operations:**\n",
        "- **Add New Customers**: Insert new customer records\n",
        "- **Update Existing Customers**: Modify customer information\n",
        "- **Delete Customers**: Remove customer records\n",
        "- **Schema Evolution**: Add new columns to the schema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Creating Snapshot 2: Adding new customers...\n",
            "✅ Snapshot 2 created: Added 200 new customers\n",
            "\n",
            "🔄 Creating Snapshot 3: Updating existing customers...\n",
            "✅ Snapshot 3 created: Updated 100 customers to Premium status\n",
            "\n",
            "🔄 Creating Snapshot 4: Deleting inactive customers...\n",
            "  Found 282 inactive customers to delete\n",
            "✅ Snapshot 4 created: Deleted inactive customers\n",
            "\n",
            "🔄 Creating Snapshot 5: Adding new column 'credit_score'...\n",
            "✅ Snapshot 5 created: Added credit_score column\n",
            "\n",
            "📸 All Snapshots Created:\n",
            "  Snapshot 1:\n",
            "    ID: 1783715834426628930\n",
            "    Timestamp: 2025-09-17 06:44:58.504000\n",
            "    Summary: [('added-files-size', '43582'), ('added-data-files', '1'), ('added-records', '1000'), ('total-data-files', '1'), ('total-delete-files', '0'), ('total-records', '1000'), ('total-files-size', '43582'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: nan\n",
            "\n",
            "  Snapshot 2:\n",
            "    ID: 332105968043556599\n",
            "    Timestamp: 2025-09-17 06:44:58.537000\n",
            "    Summary: [('added-files-size', '15792'), ('added-data-files', '1'), ('added-records', '200'), ('total-data-files', '2'), ('total-delete-files', '0'), ('total-records', '1200'), ('total-files-size', '59374'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: 1.7837158344266289e+18\n",
            "\n",
            "  Snapshot 3:\n",
            "    ID: 6503228716597911792\n",
            "    Timestamp: 2025-09-17 06:44:58.575000\n",
            "    Summary: [('removed-files-size', '59374'), ('deleted-data-files', '2'), ('deleted-records', '1200'), ('total-data-files', '0'), ('total-delete-files', '0'), ('total-records', '0'), ('total-files-size', '0'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: 3.321059680435566e+17\n",
            "\n",
            "  Snapshot 4:\n",
            "    ID: 5814724229348825479\n",
            "    Timestamp: 2025-09-17 06:44:58.587000\n",
            "    Summary: [('added-files-size', '49857'), ('added-data-files', '1'), ('added-records', '1200'), ('total-data-files', '1'), ('total-delete-files', '0'), ('total-records', '1200'), ('total-files-size', '49857'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: 6.503228716597912e+18\n",
            "\n",
            "  Snapshot 5:\n",
            "    ID: 335080899782564617\n",
            "    Timestamp: 2025-09-17 06:44:58.609000\n",
            "    Summary: [('removed-files-size', '49857'), ('deleted-data-files', '1'), ('deleted-records', '1200'), ('total-data-files', '0'), ('total-delete-files', '0'), ('total-records', '0'), ('total-files-size', '0'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: 5.814724229348825e+18\n",
            "\n",
            "  Snapshot 6:\n",
            "    ID: 5134235652662559516\n",
            "    Timestamp: 2025-09-17 06:44:58.621000\n",
            "    Summary: [('added-files-size', '43032'), ('added-data-files', '1'), ('added-records', '918'), ('total-data-files', '1'), ('total-delete-files', '0'), ('total-records', '918'), ('total-files-size', '43032'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: 3.350808997825646e+17\n",
            "\n",
            "  Snapshot 7:\n",
            "    ID: 5300982805287717163\n",
            "    Timestamp: 2025-09-17 06:44:58.648000\n",
            "    Summary: [('removed-files-size', '43032'), ('deleted-data-files', '1'), ('deleted-records', '918'), ('total-data-files', '0'), ('total-delete-files', '0'), ('total-records', '0'), ('total-files-size', '0'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: 5.13423565266256e+18\n",
            "\n",
            "  Snapshot 8:\n",
            "    ID: 5175176634825406144\n",
            "    Timestamp: 2025-09-17 06:44:58.660000\n",
            "    Summary: [('added-files-size', '43316'), ('added-data-files', '1'), ('added-records', '918'), ('total-data-files', '1'), ('total-delete-files', '0'), ('total-records', '918'), ('total-files-size', '43316'), ('total-position-deletes', '0'), ('total-equality-deletes', '0')]\n",
            "    Parent ID: 5.300982805287717e+18\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Snapshot 2: Add new customers\n",
        "print(\"🔄 Creating Snapshot 2: Adding new customers...\")\n",
        "\n",
        "# Generate additional customers\n",
        "new_customers = generate_customer_data(200)  # Add 200 new customers\n",
        "df_new_customers = pd.DataFrame(new_customers)\n",
        "\n",
        "# Convert to PyArrow table\n",
        "new_customer_table = create_customer_pyarrow_table(df_new_customers)\n",
        "\n",
        "# Append new customers (creates Snapshot 2)\n",
        "customers_table.append(new_customer_table)\n",
        "\n",
        "print(\"✅ Snapshot 2 created: Added 200 new customers\")\n",
        "\n",
        "# Snapshot 3: Update existing customers\n",
        "print(\"\\n🔄 Creating Snapshot 3: Updating existing customers...\")\n",
        "\n",
        "# Get current data\n",
        "current_data = customers_table.scan().to_arrow().to_pandas()\n",
        "\n",
        "# Update some customers (change their segment and status)\n",
        "update_indices = random.sample(range(len(current_data)), 100)  # Update 100 random customers\n",
        "current_data.loc[update_indices, 'customer_segment'] = 'Premium'\n",
        "current_data.loc[update_indices, 'account_status'] = 'Active'\n",
        "current_data.loc[update_indices, 'is_premium'] = True\n",
        "current_data.loc[update_indices, 'updated_at'] = datetime.now()\n",
        "\n",
        "# Convert back to PyArrow table\n",
        "updated_customer_table = create_customer_pyarrow_table(current_data)\n",
        "\n",
        "# Overwrite table with updated data (creates Snapshot 3)\n",
        "customers_table.overwrite(updated_customer_table)\n",
        "\n",
        "print(\"✅ Snapshot 3 created: Updated 100 customers to Premium status\")\n",
        "\n",
        "# Snapshot 4: Delete some customers\n",
        "print(\"\\n🔄 Creating Snapshot 4: Deleting inactive customers...\")\n",
        "\n",
        "# Get current data\n",
        "current_data = customers_table.scan().to_arrow().to_pandas()\n",
        "\n",
        "# Delete customers with 'Inactive' status\n",
        "inactive_customers = current_data[current_data['account_status'] == 'Inactive']\n",
        "print(f\"  Found {len(inactive_customers)} inactive customers to delete\")\n",
        "\n",
        "# Keep only active customers\n",
        "active_data = current_data[current_data['account_status'] != 'Inactive']\n",
        "\n",
        "# Convert back to PyArrow table\n",
        "active_customer_table = create_customer_pyarrow_table(active_data)\n",
        "\n",
        "# Overwrite table with active customers only (creates Snapshot 4)\n",
        "customers_table.overwrite(active_customer_table)\n",
        "\n",
        "print(\"✅ Snapshot 4 created: Deleted inactive customers\")\n",
        "\n",
        "# Snapshot 5: Schema evolution - Add new column\n",
        "print(\"\\n🔄 Creating Snapshot 5: Adding new column 'credit_score'...\")\n",
        "\n",
        "# Update schema to add credit_score column\n",
        "customers_table.update_schema().add_column(\"credit_score\", IntegerType()).commit()\n",
        "\n",
        "# Get current data and add credit_score\n",
        "current_data = customers_table.scan().to_arrow().to_pandas()\n",
        "current_data['credit_score'] = np.random.randint(300, 850, len(current_data))  # Random credit scores\n",
        "current_data['updated_at'] = datetime.now()\n",
        "\n",
        "# Convert back to PyArrow table with new schema\n",
        "pyarrow_schema_with_credit = pa.schema([\n",
        "    pa.field('customer_id', pa.string(), nullable=False),\n",
        "    pa.field('first_name', pa.string(), nullable=False),\n",
        "    pa.field('last_name', pa.string(), nullable=False),\n",
        "    pa.field('email', pa.string(), nullable=False),\n",
        "    pa.field('phone', pa.string(), nullable=False),\n",
        "    pa.field('country', pa.string(), nullable=False),\n",
        "    pa.field('city', pa.string(), nullable=False),\n",
        "    pa.field('age', pa.int32(), nullable=False),\n",
        "    pa.field('customer_segment', pa.string(), nullable=False),\n",
        "    pa.field('account_status', pa.string(), nullable=False),\n",
        "    pa.field('registration_date', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('last_login', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('total_purchases', pa.int32(), nullable=False),\n",
        "    pa.field('total_spent', pa.float64(), nullable=False),\n",
        "    pa.field('loyalty_points', pa.int32(), nullable=False),\n",
        "    pa.field('is_premium', pa.bool_(), nullable=False),\n",
        "    pa.field('newsletter_subscribed', pa.bool_(), nullable=False),\n",
        "    pa.field('created_at', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('updated_at', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('credit_score', pa.int32(), nullable=False)  # New column\n",
        "])\n",
        "\n",
        "# Convert timestamps\n",
        "timestamp_cols = ['registration_date', 'last_login', 'created_at', 'updated_at']\n",
        "for col in timestamp_cols:\n",
        "    current_data[col] = pd.to_datetime(current_data[col]).dt.floor('us')\n",
        "\n",
        "# Create table with new schema\n",
        "updated_customer_table_with_credit = pa.table({\n",
        "    'customer_id': current_data['customer_id'],\n",
        "    'first_name': current_data['first_name'],\n",
        "    'last_name': current_data['last_name'],\n",
        "    'email': current_data['email'],\n",
        "    'phone': current_data['phone'],\n",
        "    'country': current_data['country'],\n",
        "    'city': current_data['city'],\n",
        "    'age': current_data['age'],\n",
        "    'customer_segment': current_data['customer_segment'],\n",
        "    'account_status': current_data['account_status'],\n",
        "    'registration_date': current_data['registration_date'],\n",
        "    'last_login': current_data['last_login'],\n",
        "    'total_purchases': current_data['total_purchases'],\n",
        "    'total_spent': current_data['total_spent'],\n",
        "    'loyalty_points': current_data['loyalty_points'],\n",
        "    'is_premium': current_data['is_premium'],\n",
        "    'newsletter_subscribed': current_data['newsletter_subscribed'],\n",
        "    'created_at': current_data['created_at'],\n",
        "    'updated_at': current_data['updated_at'],\n",
        "    'credit_score': current_data['credit_score']\n",
        "}, schema=pyarrow_schema_with_credit)\n",
        "\n",
        "# Overwrite table with new schema (creates Snapshot 5)\n",
        "customers_table.overwrite(updated_customer_table_with_credit)\n",
        "\n",
        "print(\"✅ Snapshot 5 created: Added credit_score column\")\n",
        "\n",
        "# Display all snapshots - Safe access using helper function\n",
        "print(f\"\\n📸 All Snapshots Created:\")\n",
        "snapshots_df = customers_table.inspect.snapshots().to_pandas()\n",
        "for i, (_, snapshot) in enumerate(snapshots_df.iterrows(), 1):\n",
        "    snapshot_info = get_snapshot_info(snapshots_df, snapshot)\n",
        "    print(f\"  Snapshot {i}:\")\n",
        "    print(f\"    ID: {snapshot_info['snapshot_id']}\")\n",
        "    \n",
        "    if snapshot_info['timestamp'] is not None:\n",
        "        if isinstance(snapshot_info['timestamp'], (int, float)):\n",
        "            timestamp_str = datetime.fromtimestamp(snapshot_info['timestamp'] / 1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        else:\n",
        "            timestamp_str = str(snapshot_info['timestamp'])\n",
        "        print(f\"    Timestamp: {timestamp_str}\")\n",
        "    else:\n",
        "        print(f\"    Timestamp: Not available\")\n",
        "    \n",
        "    if snapshot_info['summary'] is not None:\n",
        "        print(f\"    Summary: {snapshot_info['summary']}\")\n",
        "    else:\n",
        "        print(f\"    Summary: Not available\")\n",
        "    \n",
        "    # Try to get parent snapshot ID\n",
        "    parent_cols = ['parent_snapshot_id', 'parent_id', 'parent']\n",
        "    parent_id = None\n",
        "    for col in parent_cols:\n",
        "        if col in snapshots_df.columns:\n",
        "            parent_id = snapshot[col]\n",
        "            break\n",
        "    \n",
        "    if parent_id is not None:\n",
        "        print(f\"    Parent ID: {parent_id}\")\n",
        "    else:\n",
        "        print(f\"    Parent ID: Not available\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Time Travel Queries - Querying Historical Data\n",
        "\n",
        "Now let's explore the power of time travel queries! We can query data as it existed at any point in time using snapshot IDs or timestamps.\n",
        "\n",
        "### ⏰ **Time Travel Methods:**\n",
        "- **Snapshot ID**: Query data at a specific snapshot\n",
        "- **Timestamp**: Query data at a specific point in time\n",
        "- **AS OF**: Use SQL-style time travel syntax\n",
        "- **Historical Analysis**: Compare data across different time periods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📸 Available Snapshots for Time Travel:\n",
            "  Snapshot 1: ID=1783715834426628930, Time=2025-09-17 06:44:58.504000\n",
            "  Snapshot 2: ID=332105968043556599, Time=2025-09-17 06:44:58.537000\n",
            "  Snapshot 3: ID=6503228716597911792, Time=2025-09-17 06:44:58.575000\n",
            "  Snapshot 4: ID=5814724229348825479, Time=2025-09-17 06:44:58.587000\n",
            "  Snapshot 5: ID=335080899782564617, Time=2025-09-17 06:44:58.609000\n",
            "  Snapshot 6: ID=5134235652662559516, Time=2025-09-17 06:44:58.621000\n",
            "  Snapshot 7: ID=5300982805287717163, Time=2025-09-17 06:44:58.648000\n",
            "  Snapshot 8: ID=5175176634825406144, Time=2025-09-17 06:44:58.660000\n",
            "\n",
            "⏰ Time Travel Query 1: Data at Snapshot 1 (Original)\n",
            "  Records at Snapshot 1: 1,000\n",
            "  Columns at Snapshot 1: ['customer_id', 'first_name', 'last_name', 'email', 'phone', 'country', 'city', 'age', 'customer_segment', 'account_status', 'registration_date', 'last_login', 'total_purchases', 'total_spent', 'loyalty_points', 'is_premium', 'newsletter_subscribed', 'created_at', 'updated_at']\n",
            "  Premium customers at Snapshot 1: 259\n",
            "\n",
            "⏰ Time Travel Query 2: Data at Snapshot 2 (After Adding Customers)\n",
            "  Records at Snapshot 2: 1,200\n",
            "  New customers added: 200\n",
            "\n",
            "⏰ Time Travel Query 3: Data at Snapshot 3 (After Updates)\n",
            "  Records at Snapshot 3: 0\n",
            "  Premium customers at Snapshot 3: 0\n",
            "\n",
            "⏰ Time Travel Query 4: Data at Snapshot 4 (After Deletions)\n",
            "  Records at Snapshot 4: 1,200\n",
            "  Customers deleted: -1,200\n",
            "  Inactive customers at Snapshot 4: 282\n",
            "\n",
            "⏰ Time Travel Query 5: Data at Snapshot 5 (With New Schema)\n",
            "  Records at Snapshot 5: 0\n",
            "  Columns at Snapshot 5: ['customer_id', 'first_name', 'last_name', 'email', 'phone', 'country', 'city', 'age', 'customer_segment', 'account_status', 'registration_date', 'last_login', 'total_purchases', 'total_spent', 'loyalty_points', 'is_premium', 'newsletter_subscribed', 'created_at', 'updated_at']\n",
            "  Has credit_score column: False\n",
            "\n",
            "⏰ Time Travel Query 6: Current Data (Latest Snapshot)\n",
            "  Current records: 918\n",
            "  Current columns: ['customer_id', 'first_name', 'last_name', 'email', 'phone', 'country', 'city', 'age', 'customer_segment', 'account_status', 'registration_date', 'last_login', 'total_purchases', 'total_spent', 'loyalty_points', 'is_premium', 'newsletter_subscribed', 'created_at', 'updated_at', 'credit_score']\n",
            "\n",
            "📊 Data Evolution Summary:\n",
            "------------------------------------------------------------\n",
            "Snapshot     Records    Premium    Inactive   Columns   \n",
            "------------------------------------------------------------\n",
            "Snapshot 1        1,000      259        256        19        \n",
            "Snapshot 2        1,200      315        304        19        \n",
            "Snapshot 3        0          0          0          19        \n",
            "Snapshot 4        1,200      388        282        19        \n",
            "Snapshot 5        0          0          0          19        \n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Get snapshot information for time travel - Safe access\n",
        "snapshots_df = customers_table.inspect.snapshots().to_pandas()\n",
        "print(\"📸 Available Snapshots for Time Travel:\")\n",
        "for i, (_, snapshot) in enumerate(snapshots_df.iterrows(), 1):\n",
        "    snapshot_info = get_snapshot_info(snapshots_df, snapshot)\n",
        "    if snapshot_info['timestamp'] is not None:\n",
        "        if isinstance(snapshot_info['timestamp'], (int, float)):\n",
        "            timestamp_str = datetime.fromtimestamp(snapshot_info['timestamp'] / 1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        else:\n",
        "            timestamp_str = str(snapshot_info['timestamp'])\n",
        "    else:\n",
        "        timestamp_str = \"Not available\"\n",
        "    print(f\"  Snapshot {i}: ID={snapshot_info['snapshot_id']}, Time={timestamp_str}\")\n",
        "\n",
        "# Time Travel Query 1: Query data at Snapshot 1 (original data)\n",
        "print(f\"\\n⏰ Time Travel Query 1: Data at Snapshot 1 (Original)\")\n",
        "snapshot_1_id = snapshots_df.iloc[0]['snapshot_id']\n",
        "snapshot_1_data = customers_table.scan(snapshot_id=snapshot_1_id).to_arrow().to_pandas()\n",
        "\n",
        "print(f\"  Records at Snapshot 1: {len(snapshot_1_data):,}\")\n",
        "print(f\"  Columns at Snapshot 1: {list(snapshot_1_data.columns)}\")\n",
        "print(f\"  Premium customers at Snapshot 1: {len(snapshot_1_data[snapshot_1_data['customer_segment'] == 'Premium']):,}\")\n",
        "\n",
        "# Time Travel Query 2: Query data at Snapshot 2 (after adding customers)\n",
        "print(f\"\\n⏰ Time Travel Query 2: Data at Snapshot 2 (After Adding Customers)\")\n",
        "snapshot_2_id = snapshots_df.iloc[1]['snapshot_id']\n",
        "snapshot_2_data = customers_table.scan(snapshot_id=snapshot_2_id).to_arrow().to_pandas()\n",
        "\n",
        "print(f\"  Records at Snapshot 2: {len(snapshot_2_data):,}\")\n",
        "print(f\"  New customers added: {len(snapshot_2_data) - len(snapshot_1_data):,}\")\n",
        "\n",
        "# Time Travel Query 3: Query data at Snapshot 3 (after updates)\n",
        "print(f\"\\n⏰ Time Travel Query 3: Data at Snapshot 3 (After Updates)\")\n",
        "snapshot_3_id = snapshots_df.iloc[2]['snapshot_id']\n",
        "snapshot_3_data = customers_table.scan(snapshot_id=snapshot_3_id).to_arrow().to_pandas()\n",
        "\n",
        "print(f\"  Records at Snapshot 3: {len(snapshot_3_data):,}\")\n",
        "print(f\"  Premium customers at Snapshot 3: {len(snapshot_3_data[snapshot_3_data['customer_segment'] == 'Premium']):,}\")\n",
        "\n",
        "# Time Travel Query 4: Query data at Snapshot 4 (after deletions)\n",
        "print(f\"\\n⏰ Time Travel Query 4: Data at Snapshot 4 (After Deletions)\")\n",
        "snapshot_4_id = snapshots_df.iloc[3]['snapshot_id']\n",
        "snapshot_4_data = customers_table.scan(snapshot_id=snapshot_4_id).to_arrow().to_pandas()\n",
        "\n",
        "print(f\"  Records at Snapshot 4: {len(snapshot_4_data):,}\")\n",
        "print(f\"  Customers deleted: {len(snapshot_3_data) - len(snapshot_4_data):,}\")\n",
        "print(f\"  Inactive customers at Snapshot 4: {len(snapshot_4_data[snapshot_4_data['account_status'] == 'Inactive']):,}\")\n",
        "\n",
        "# Time Travel Query 5: Query data at Snapshot 5 (with new schema)\n",
        "print(f\"\\n⏰ Time Travel Query 5: Data at Snapshot 5 (With New Schema)\")\n",
        "snapshot_5_id = snapshots_df.iloc[4]['snapshot_id']\n",
        "snapshot_5_data = customers_table.scan(snapshot_id=snapshot_5_id).to_arrow().to_pandas()\n",
        "\n",
        "print(f\"  Records at Snapshot 5: {len(snapshot_5_data):,}\")\n",
        "print(f\"  Columns at Snapshot 5: {list(snapshot_5_data.columns)}\")\n",
        "print(f\"  Has credit_score column: {'credit_score' in snapshot_5_data.columns}\")\n",
        "if 'credit_score' in snapshot_5_data.columns:\n",
        "    print(f\"  Average credit score: {snapshot_5_data['credit_score'].mean():.1f}\")\n",
        "\n",
        "# Time Travel Query 6: Query current data (latest snapshot)\n",
        "print(f\"\\n⏰ Time Travel Query 6: Current Data (Latest Snapshot)\")\n",
        "current_data = customers_table.scan().to_arrow().to_pandas()\n",
        "\n",
        "print(f\"  Current records: {len(current_data):,}\")\n",
        "print(f\"  Current columns: {list(current_data.columns)}\")\n",
        "\n",
        "# Compare data across snapshots\n",
        "print(f\"\\n📊 Data Evolution Summary:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Snapshot':<12} {'Records':<10} {'Premium':<10} {'Inactive':<10} {'Columns':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "snapshot_data_list = [snapshot_1_data, snapshot_2_data, snapshot_3_data, snapshot_4_data, snapshot_5_data]\n",
        "for i, data in enumerate(snapshot_data_list, 1):\n",
        "    premium_count = len(data[data['customer_segment'] == 'Premium'])\n",
        "    inactive_count = len(data[data['account_status'] == 'Inactive'])\n",
        "    column_count = len(data.columns)\n",
        "    \n",
        "    print(f\"Snapshot {i:<8} {len(data):<10,} {premium_count:<10,} {inactive_count:<10,} {column_count:<10}\")\n",
        "\n",
        "print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Rollback Operations - Restoring Previous Versions\n",
        "\n",
        "### **How Rollback Works in PyIceberg:**\n",
        "\n",
        "Unlike traditional databases, PyIceberg doesn't have a direct `rollback_to_snapshot()` method. Instead, rollback is achieved by:\n",
        "\n",
        "1. **Query Historical Data**: Use `scan(snapshot_id=snapshot_id)` to get data from a specific snapshot\n",
        "2. **Overwrite Current Table**: Use `overwrite()` to replace current data with historical data\n",
        "3. **Create New Snapshot**: The overwrite operation creates a new snapshot with the restored data\n",
        "\n",
        "### **Rollback Strategies:**\n",
        "\n",
        "- **Data Rollback**: Restore data to a previous state\n",
        "- **Schema Rollback**: Restore schema to a previous version (requires schema evolution)\n",
        "- **Complete Rollback**: Restore both data and schema to a previous snapshot\n",
        "\n",
        "### **Important Notes:**\n",
        "\n",
        "- Rollback creates new snapshots (doesn't delete existing ones)\n",
        "- Historical snapshots remain accessible for time travel\n",
        "- Schema rollback requires careful handling of column compatibility\n",
        "- Rollback operations are atomic and consistent\n",
        "\n",
        "### **Use Cases:**\n",
        "\n",
        "- **Data Recovery**: Restore accidentally deleted or corrupted data\n",
        "- **Testing**: Rollback to test different scenarios\n",
        "- **Compliance**: Meet regulatory requirements for data restoration\n",
        "- **Debugging**: Investigate issues by examining historical states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Rollback Operations - Restoring Previous Versions\n",
        "\n",
        "One of the most powerful features of Iceberg is the ability to rollback to previous snapshots. This is essential for data recovery and debugging.\n",
        "\n",
        "### 🔄 **Rollback Scenarios:**\n",
        "- **Data Recovery**: Restore accidentally deleted data\n",
        "- **Bug Fixes**: Rollback to a known good state\n",
        "- **Schema Rollback**: Undo schema changes\n",
        "- **Testing**: Rollback for testing different scenarios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Rollback Operation 1: Rolling back to Snapshot 3 (before deletions)\n",
            "  Current records before rollback: 1,000\n",
            "✅ Rollback completed!\n",
            "  Records after rollback: 0\n",
            "  Records restored: -1,000\n",
            "  Inactive customers restored: 0\n",
            "\n",
            "🔄 Rollback Operation 2: Rolling back to Snapshot 1 (original state)\n",
            "  Current records before rollback: 0\n",
            "✅ Rollback to original state completed!\n",
            "  Records after rollback: 1,000\n",
            "  Records restored: 1,000\n",
            "  Columns after rollback: ['customer_id', 'first_name', 'last_name', 'email', 'phone', 'country', 'city', 'age', 'customer_segment', 'account_status', 'registration_date', 'last_login', 'total_purchases', 'total_spent', 'loyalty_points', 'is_premium', 'newsletter_subscribed', 'created_at', 'updated_at', 'credit_score']\n",
            "  Has credit_score column: True\n",
            "  Premium customers restored: 259\n",
            "\n",
            "🔄 Rollback Operation 3: Rolling back to Snapshot 5 (latest with all features)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages/pyiceberg/table/__init__.py:715: UserWarning: Delete operation did not match any records\n",
            "  warnings.warn(\"Delete operation did not match any records\")\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'credit_score'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniforge3/envs/datalab/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'credit_score'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 114\u001b[0m\n\u001b[1;32m     91\u001b[0m     snapshot_5_data[col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(snapshot_5_data[col])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Create table with credit_score schema\u001b[39;00m\n\u001b[1;32m     94\u001b[0m snapshot_5_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mtable({\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_name\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_name\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphone\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphone\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_segment\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_segment\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_status\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_status\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistration_date\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistration_date\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_login\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_login\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_purchases\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_purchases\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_spent\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_spent\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_points\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloyalty_points\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_premium\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_premium\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewsletter_subscribed\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewsletter_subscribed\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdated_at\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot_5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdated_at\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredit_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43msnapshot_5_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcredit_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    115\u001b[0m }, schema\u001b[38;5;241m=\u001b[39mpyarrow_schema_with_credit)\n\u001b[1;32m    117\u001b[0m customers_table\u001b[38;5;241m.\u001b[39moverwrite(snapshot_5_table)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Rollback to latest state completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/envs/datalab/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/miniforge3/envs/datalab/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'credit_score'"
          ]
        }
      ],
      "source": [
        "# Rollback Operation 1: Rollback to Snapshot 3 (before deletions)\n",
        "print(\"🔄 Rollback Operation 1: Rolling back to Snapshot 3 (before deletions)\")\n",
        "\n",
        "# Get current state before rollback\n",
        "current_before_rollback = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Current records before rollback: {len(current_before_rollback):,}\")\n",
        "\n",
        "# Get data from Snapshot 3 and overwrite current table\n",
        "snapshot_3_id = snapshots_df.iloc[2]['snapshot_id']\n",
        "snapshot_3_data = customers_table.scan(snapshot_id=snapshot_3_id).to_arrow().to_pandas()\n",
        "\n",
        "# Convert back to PyArrow table for overwrite\n",
        "snapshot_3_table = create_customer_pyarrow_table(snapshot_3_data)\n",
        "customers_table.overwrite(snapshot_3_table)\n",
        "\n",
        "print(\"✅ Rollback completed!\")\n",
        "\n",
        "# Verify rollback\n",
        "current_after_rollback = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Records after rollback: {len(current_after_rollback):,}\")\n",
        "print(f\"  Records restored: {len(current_after_rollback) - len(current_before_rollback):,}\")\n",
        "\n",
        "# Check if deleted customers are back\n",
        "inactive_customers_restored = len(current_after_rollback[current_after_rollback['account_status'] == 'Inactive'])\n",
        "print(f\"  Inactive customers restored: {inactive_customers_restored:,}\")\n",
        "\n",
        "# Rollback Operation 2: Rollback to Snapshot 1 (original state)\n",
        "print(f\"\\n🔄 Rollback Operation 2: Rolling back to Snapshot 1 (original state)\")\n",
        "\n",
        "# Get current state\n",
        "current_before_rollback2 = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Current records before rollback: {len(current_before_rollback2):,}\")\n",
        "\n",
        "# Get data from Snapshot 1 and overwrite current table\n",
        "snapshot_1_id = snapshots_df.iloc[0]['snapshot_id']\n",
        "snapshot_1_data = customers_table.scan(snapshot_id=snapshot_1_id).to_arrow().to_pandas()\n",
        "\n",
        "# Convert back to PyArrow table for overwrite\n",
        "snapshot_1_table = create_customer_pyarrow_table(snapshot_1_data)\n",
        "customers_table.overwrite(snapshot_1_table)\n",
        "\n",
        "print(\"✅ Rollback to original state completed!\")\n",
        "\n",
        "# Verify rollback\n",
        "current_after_rollback2 = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Records after rollback: {len(current_after_rollback2):,}\")\n",
        "print(f\"  Records restored: {len(current_after_rollback2) - len(current_before_rollback2):,}\")\n",
        "\n",
        "# Check if schema is back to original\n",
        "print(f\"  Columns after rollback: {list(current_after_rollback2.columns)}\")\n",
        "print(f\"  Has credit_score column: {'credit_score' in current_after_rollback2.columns}\")\n",
        "\n",
        "# Check premium customers (should be back to original count)\n",
        "premium_customers_restored = len(current_after_rollback2[current_after_rollback2['customer_segment'] == 'Premium'])\n",
        "print(f\"  Premium customers restored: {premium_customers_restored:,}\")\n",
        "\n",
        "# Rollback Operation 3: Rollback to Snapshot 5 (latest with all features)\n",
        "print(f\"\\n🔄 Rollback Operation 3: Rolling back to Snapshot 5 (latest with all features)\")\n",
        "\n",
        "# Get data from Snapshot 5 and overwrite current table\n",
        "snapshot_5_id = snapshots_df.iloc[4]['snapshot_id']\n",
        "snapshot_5_data = customers_table.scan(snapshot_id=snapshot_5_id).to_arrow().to_pandas()\n",
        "\n",
        "# Convert back to PyArrow table for overwrite (with credit_score column)\n",
        "pyarrow_schema_with_credit = pa.schema([\n",
        "    pa.field('customer_id', pa.string(), nullable=False),\n",
        "    pa.field('first_name', pa.string(), nullable=False),\n",
        "    pa.field('last_name', pa.string(), nullable=False),\n",
        "    pa.field('email', pa.string(), nullable=False),\n",
        "    pa.field('phone', pa.string(), nullable=False),\n",
        "    pa.field('country', pa.string(), nullable=False),\n",
        "    pa.field('city', pa.string(), nullable=False),\n",
        "    pa.field('age', pa.int32(), nullable=False),\n",
        "    pa.field('customer_segment', pa.string(), nullable=False),\n",
        "    pa.field('account_status', pa.string(), nullable=False),\n",
        "    pa.field('registration_date', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('last_login', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('total_purchases', pa.int32(), nullable=False),\n",
        "    pa.field('total_spent', pa.float64(), nullable=False),\n",
        "    pa.field('loyalty_points', pa.int32(), nullable=False),\n",
        "    pa.field('is_premium', pa.bool_(), nullable=False),\n",
        "    pa.field('newsletter_subscribed', pa.bool_(), nullable=False),\n",
        "    pa.field('created_at', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('updated_at', pa.timestamp('us'), nullable=False),\n",
        "    pa.field('credit_score', pa.int32(), nullable=False)\n",
        "])\n",
        "\n",
        "# Convert timestamps\n",
        "timestamp_cols = ['registration_date', 'last_login', 'created_at', 'updated_at']\n",
        "for col in timestamp_cols:\n",
        "    snapshot_5_data[col] = pd.to_datetime(snapshot_5_data[col]).dt.floor('us')\n",
        "\n",
        "# Create table with credit_score schema\n",
        "snapshot_5_table = pa.table({\n",
        "    'customer_id': snapshot_5_data['customer_id'],\n",
        "    'first_name': snapshot_5_data['first_name'],\n",
        "    'last_name': snapshot_5_data['last_name'],\n",
        "    'email': snapshot_5_data['email'],\n",
        "    'phone': snapshot_5_data['phone'],\n",
        "    'country': snapshot_5_data['country'],\n",
        "    'city': snapshot_5_data['city'],\n",
        "    'age': snapshot_5_data['age'],\n",
        "    'customer_segment': snapshot_5_data['customer_segment'],\n",
        "    'account_status': snapshot_5_data['account_status'],\n",
        "    'registration_date': snapshot_5_data['registration_date'],\n",
        "    'last_login': snapshot_5_data['last_login'],\n",
        "    'total_purchases': snapshot_5_data['total_purchases'],\n",
        "    'total_spent': snapshot_5_data['total_spent'],\n",
        "    'loyalty_points': snapshot_5_data['loyalty_points'],\n",
        "    'is_premium': snapshot_5_data['is_premium'],\n",
        "    'newsletter_subscribed': snapshot_5_data['newsletter_subscribed'],\n",
        "    'created_at': snapshot_5_data['created_at'],\n",
        "    'updated_at': snapshot_5_data['updated_at'],\n",
        "    'credit_score': snapshot_5_data['credit_score']\n",
        "}, schema=pyarrow_schema_with_credit)\n",
        "\n",
        "customers_table.overwrite(snapshot_5_table)\n",
        "\n",
        "print(\"✅ Rollback to latest state completed!\")\n",
        "\n",
        "# Verify rollback\n",
        "final_state = customers_table.scan().to_arrow().to_pandas()\n",
        "print(f\"  Final records: {len(final_state):,}\")\n",
        "print(f\"  Final columns: {list(final_state.columns)}\")\n",
        "print(f\"  Has credit_score column: {'credit_score' in final_state.columns}\")\n",
        "\n",
        "# Display rollback summary\n",
        "print(f\"\\n📊 Rollback Operations Summary:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Rollback 1 (to Snapshot 3):\")\n",
        "print(f\"  Restored {len(current_after_rollback) - len(current_before_rollback):,} deleted customers\")\n",
        "print(f\"  Restored {inactive_customers_restored:,} inactive customers\")\n",
        "\n",
        "print(f\"\\nRollback 2 (to Snapshot 1):\")\n",
        "print(f\"  Restored {len(current_after_rollback2) - len(current_before_rollback2):,} customers\")\n",
        "print(f\"  Removed credit_score column\")\n",
        "print(f\"  Reset premium customers to {premium_customers_restored:,}\")\n",
        "\n",
        "print(f\"\\nRollback 3 (to Snapshot 5):\")\n",
        "print(f\"  Restored to latest state with all features\")\n",
        "print(f\"  Final records: {len(final_state):,}\")\n",
        "print(f\"  Final columns: {len(final_state.columns)}\")\n",
        "\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Lab Summary & Best Practices\n",
        "\n",
        "### 🎉 **Congratulations!**\n",
        "\n",
        "You've completed the **Time Travel & Versioning Lab** and learned about:\n",
        "\n",
        "✅ **Snapshot Management**: Creating and managing data snapshots  \n",
        "✅ **Time Travel Queries**: Querying data at specific points in time  \n",
        "✅ **Rollback Operations**: Restoring data to previous versions  \n",
        "✅ **Version Comparison**: Comparing data between different versions  \n",
        "✅ **Historical Analysis**: Analyzing data changes over time  \n",
        "✅ **Production Scenarios**: Applying time travel to real-world use cases  \n",
        "\n",
        "### 🚀 **Key Takeaways:**\n",
        "\n",
        "#### **1. Time Travel is Powerful:**\n",
        "- **Data Recovery**: Restore accidentally deleted or corrupted data\n",
        "- **Audit Trails**: Track all changes for compliance and auditing\n",
        "- **A/B Testing**: Compare different versions of datasets\n",
        "- **Debugging**: Investigate data issues by examining historical states\n",
        "- **Compliance**: Meet regulatory requirements for data retention\n",
        "\n",
        "#### **2. Snapshot Management:**\n",
        "- **Every Write Creates a Snapshot**: Each operation creates a new version\n",
        "- **Snapshot IDs**: Unique identifiers for each version\n",
        "- **Timestamps**: Track when each snapshot was created\n",
        "- **Parent-Child Relationships**: Track snapshot lineage\n",
        "- **Metadata**: Rich metadata about each operation\n",
        "\n",
        "#### **3. Rollback Capabilities:**\n",
        "- **Instant Rollback**: Restore data to any previous state\n",
        "- **Schema Rollback**: Undo schema changes\n",
        "- **Data Recovery**: Restore deleted or modified data\n",
        "- **Testing**: Rollback for testing different scenarios\n",
        "- **Production Safety**: Safe rollback in production environments\n",
        "\n",
        "### 📚 **Production Implementation:**\n",
        "\n",
        "#### **1. Apache Spark:**\n",
        "```sql\n",
        "-- Time travel queries with Spark\n",
        "SELECT * FROM catalog.database.table VERSION AS OF 1234567890;\n",
        "SELECT * FROM catalog.database.table TIMESTAMP AS OF '2023-01-01 12:00:00';\n",
        "\n",
        "-- Rollback with Spark\n",
        "CALL system.rollback_to_snapshot('catalog.database.table', 1234567890);\n",
        "```\n",
        "\n",
        "#### **2. Trino:**\n",
        "```sql\n",
        "-- Time travel queries with Trino\n",
        "SELECT * FROM catalog.database.table FOR VERSION AS OF 1234567890;\n",
        "SELECT * FROM catalog.database.table FOR TIMESTAMP AS OF TIMESTAMP '2023-01-01 12:00:00';\n",
        "\n",
        "-- Rollback with Trino\n",
        "CALL system.rollback_to_snapshot('catalog.database.table', 1234567890);\n",
        "```\n",
        "\n",
        "#### **3. PyIceberg:**\n",
        "```python\n",
        "# Time travel queries with PyIceberg\n",
        "table.scan(snapshot_id=1234567890).to_arrow()\n",
        "table.scan(as_of_timestamp=datetime(2023, 1, 1)).to_arrow()\n",
        "\n",
        "# Rollback with PyIceberg\n",
        "table.rollback_to_snapshot(1234567890)\n",
        "```\n",
        "\n",
        "### 🎯 **Best Practices:**\n",
        "\n",
        "#### **1. Snapshot Management:**\n",
        "- **Regular Cleanup**: Remove old snapshots to save storage\n",
        "- **Snapshot Retention**: Keep snapshots for required retention period\n",
        "- **Metadata Tracking**: Track snapshot purposes and descriptions\n",
        "- **Access Control**: Control who can create and rollback snapshots\n",
        "\n",
        "#### **2. Time Travel Queries:**\n",
        "- **Performance**: Time travel queries may be slower than current queries\n",
        "- **Storage**: Historical data consumes storage space\n",
        "- **Indexing**: Consider indexing strategies for historical queries\n",
        "- **Caching**: Cache frequently accessed historical data\n",
        "\n",
        "#### **3. Rollback Operations:**\n",
        "- **Testing**: Always test rollback operations in non-production\n",
        "- **Backup**: Keep backups of critical snapshots\n",
        "- **Documentation**: Document rollback procedures and scenarios\n",
        "- **Monitoring**: Monitor rollback operations and their impact\n",
        "\n",
        "#### **4. Production Scenarios:**\n",
        "- **Data Recovery**: Use for accidental data deletion recovery\n",
        "- **Bug Fixes**: Rollback to known good states after bugs\n",
        "- **Schema Evolution**: Rollback schema changes if needed\n",
        "- **Compliance**: Meet regulatory requirements for data retention\n",
        "\n",
        "### 🔍 **Advanced Topics:**\n",
        "\n",
        "#### **1. Snapshot Expiration:**\n",
        "- **Automatic Cleanup**: Configure automatic snapshot expiration\n",
        "- **Retention Policies**: Set retention policies for different snapshot types\n",
        "- **Storage Optimization**: Optimize storage by removing old snapshots\n",
        "- **Cost Management**: Manage costs by controlling snapshot retention\n",
        "\n",
        "#### **2. Branching and Tagging:**\n",
        "- **Snapshot Branches**: Create branches for different data versions\n",
        "- **Snapshot Tags**: Tag important snapshots for easy reference\n",
        "- **Merge Operations**: Merge branches back to main branch\n",
        "- **Conflict Resolution**: Handle conflicts during merge operations\n",
        "\n",
        "#### **3. Performance Optimization:**\n",
        "- **Snapshot Pruning**: Remove unnecessary snapshots\n",
        "- **Compaction**: Compact historical data for better performance\n",
        "- **Partitioning**: Use partitioning for better historical query performance\n",
        "- **Caching**: Cache frequently accessed historical data\n",
        "\n",
        "### 🎯 **Next Steps:**\n",
        "\n",
        "1. **Implement in Production**: Apply time travel to your data lakes\n",
        "2. **Set Up Monitoring**: Monitor snapshot creation and rollback operations\n",
        "3. **Create Policies**: Establish snapshot retention and cleanup policies\n",
        "4. **Train Teams**: Train teams on time travel capabilities and best practices\n",
        "5. **Test Scenarios**: Test various rollback and recovery scenarios\n",
        "\n",
        "### 📖 **Additional Resources:**\n",
        "\n",
        "- [Apache Iceberg Time Travel Documentation](https://iceberg.apache.org/docs/latest/spark-configuration/#time-travel)\n",
        "- [Spark SQL Time Travel](https://spark.apache.org/docs/latest/sql-data-sources-iceberg.html#time-travel)\n",
        "- [Trino Iceberg Time Travel](https://trino.io/docs/current/connector/iceberg.html#time-travel)\n",
        "- [PyIceberg Time Travel](https://py.iceberg.apache.org/operations/time-travel/)\n",
        "\n",
        "**Happy time traveling! ⏰🚀**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
